{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPOCEyNFMjoZ1vk5xfaTGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ying2sun/Capstone-Project-Salifort-Motors/blob/main/final_analysis_and_plots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capstone Project: Video Summarization via Knowledge Distillation\n",
        "\n",
        "This notebook contains the reproducible code for fine-tuning a **Flan-T5** model using a synthetic dataset generated by Gemini 2.0. It includes the data loading, training pipeline, inference logic, and quantitative evaluation (ROUGE and Semantic Similarity).\n",
        "\n",
        "**Prerequisites:**\n",
        "Ensure the training dataset file `gold_dataset_merged_final.csv` is present in the current working directory."
      ],
      "metadata": {
        "id": "fmzal9ura6Dr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3kJZTSda36d"
      },
      "outputs": [],
      "source": [
        "# 1. Environment Setup & Dependencies\n",
        "# Installing necessary libraries for transformer models, vector databases, and evaluation metrics.\n",
        "\n",
        "!pip install -q transformers>=4.30.0 torch>=2.0.0 youtube-transcript-api sentencepiece pandas numpy seaborn matplotlib scikit-learn rouge-score sentence-transformers datasets evaluate accelerate\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Acquisition & Preprocessing\n",
        "\n",
        "Loading the YouTube Transcriptions dataset and preparing it for fine-tuning. We utilize a pre-generated \"Gold Standard\" dataset (distilled from Gemini 2.0) to overcome the lack of labeled summaries."
      ],
      "metadata": {
        "id": "szTyJ97vbcLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "DATA_FILENAME = \"gold_dataset_merged_final.csv\"\n",
        "MODEL_CHECKPOINT = \"google/flan-t5-base\"\n",
        "\n",
        "# 2.1 Load Data\n",
        "if os.path.exists(DATA_FILENAME):\n",
        "    print(f\"[INFO] Loading dataset from {DATA_FILENAME}...\")\n",
        "    df = pd.read_csv(DATA_FILENAME)\n",
        "    # Ensure no missing values in critical columns\n",
        "    df = df.dropna(subset=['target_summary', 'full_transcript'])\n",
        "    print(f\"[INFO] Dataset loaded. Total samples: {len(df)}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"The file {DATA_FILENAME} was not found. Please upload it to the runtime.\")\n",
        "\n",
        "# 2.2 Split Data\n",
        "# Convert to Hugging Face Dataset format and split 10% for validation/testing\n",
        "dataset = Dataset.from_pandas(df[['full_transcript', 'target_summary']])\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(\"[INFO] Data split completed:\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "uTo1ojjIbg82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tokenization & Model Setup\n",
        "\n",
        "Preparing the data for the Seq2Seq model by tokenizing inputs and targets, and initializing the Flan-T5-Base model."
      ],
      "metadata": {
        "id": "ruJXtQ1-bva5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes inputs and targets.\n",
        "    Truncates inputs to 1024 tokens to fit model context.\n",
        "    \"\"\"\n",
        "    # T5 models expect a task prefix\n",
        "    inputs = [\"summarize: \" + str(doc) for doc in examples[\"full_transcript\"]]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Tokenize targets (labels)\n",
        "    labels = tokenizer(text_target=examples[\"target_summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['full_transcript', 'target_summary'])\n",
        "\n",
        "# 3.2 Initialize Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
      ],
      "metadata": {
        "id": "4eA-ZABAbz38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fine-Tuning (Knowledge Distillation)\n",
        "\n",
        "Training the model using `Seq2SeqTrainer`. We use mixed-precision (FP16) and `Adafactor` optimizer for memory efficiency."
      ],
      "metadata": {
        "id": "dW2KVNh4b2-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Define Training Arguments\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"flan_t5_base_video_summarizer\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    fp16=torch.cuda.is_available(), # Use mixed precision if on GPU\n",
        "    optim=\"adafactor\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 4.2 Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# 4.3 Start Training\n",
        "print(\"[INFO] Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# 4.4 Save Artifacts\n",
        "save_path = \"final_flan_t5_model\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"[INFO] Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "d3jkpOJ_b5Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Inference Pipeline\n",
        "\n",
        "Generating summaries for the validation set using the fine-tuned model to assess performance."
      ],
      "metadata": {
        "id": "b_9QjGkxb9GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Reload Fine-Tuned Model\n",
        "print(\"[INFO] Reloading model for inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(save_path).to(device)\n",
        "\n",
        "# 5.2 Utility Functions\n",
        "def clean_text_for_inference(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning.\n",
        "    \"\"\"\n",
        "    if not text: return \"\"\n",
        "    # Normalize whitespace\n",
        "    clean = text.replace(\"\\n\", \" \")\n",
        "    # Truncate to approx 4000 chars to fit context window\n",
        "    return clean[:4000]\n",
        "\n",
        "def generate_summary(transcript):\n",
        "    input_text = \"summarize: \" + clean_text_for_inference(transcript)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=1024,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=150,\n",
        "            min_length=40,\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Capitalize first letter\n",
        "    return summary[0].upper() + summary[1:] if summary else summary\n",
        "\n",
        "# 5.3 Run Inference on Evaluation Set\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "print(f\"[INFO] Generating summaries for {len(test_df)} validation samples...\")\n",
        "\n",
        "generated_summaries = []\n",
        "for transcript in tqdm(test_df['full_transcript']):\n",
        "    generated_summaries.append(generate_summary(transcript))\n",
        "\n",
        "test_df['generated_summary'] = generated_summaries"
      ],
      "metadata": {
        "id": "ZirWXj4cb_Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Quantitative Evaluation\n",
        "\n",
        "Assessing model performance using ROUGE (Lexical Overlap) and Cosine Similarity (Semantic Understanding)."
      ],
      "metadata": {
        "id": "vTWfFeeccDHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 ROUGE Score Calculation\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Filter out potential empty results\n",
        "valid_indices = [i for i, s in enumerate(test_df['generated_summary']) if s]\n",
        "candidates = [test_df['generated_summary'][i] for i in valid_indices]\n",
        "references = [test_df['target_summary'][i] for i in valid_indices]\n",
        "\n",
        "rouge_results = rouge.compute(predictions=candidates, references=references, use_stemmer=True)\n",
        "\n",
        "print(\"\\n=== ROUGE Scores (Lexical) ===\")\n",
        "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
        "\n",
        "# 6.2 Semantic Similarity Calculation\n",
        "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"\\n[INFO] Computing semantic embeddings...\")\n",
        "cand_embeddings = sim_model.encode(candidates, convert_to_tensor=True)\n",
        "ref_embeddings = sim_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "cosine_scores = []\n",
        "for i in range(len(candidates)):\n",
        "    score = util.cos_sim(cand_embeddings[i], ref_embeddings[i]).item()\n",
        "    cosine_scores.append(score)\n",
        "\n",
        "avg_semantic_score = np.mean(cosine_scores)\n",
        "print(f\"\\n=== Semantic Similarity Score ===\")\n",
        "print(f\"Average Cosine Similarity: {avg_semantic_score:.4f}\")"
      ],
      "metadata": {
        "id": "kNtNs2mOcF8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Visualizations\n",
        "\n",
        "Visualizing the trade-off between transcript length and performance, and the gap between lexical vs. semantic understanding."
      ],
      "metadata": {
        "id": "rxy_GxKgcImE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to count words\n",
        "def count_words(text):\n",
        "    return len(text.split()) if isinstance(text, str) else 0\n",
        "\n",
        "# 7.1 Visual 1: ROUGE Score vs Transcript Length\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "transcript_lengths = []\n",
        "instance_rouge_scores = []\n",
        "\n",
        "# Get source transcripts corresponding to valid predictions\n",
        "source_texts = [test_df['full_transcript'][i] for i in valid_indices]\n",
        "\n",
        "for src, pred, ref in zip(source_texts, candidates, references):\n",
        "    transcript_lengths.append(count_words(src))\n",
        "    instance_rouge_scores.append(scorer.score(ref, pred)['rouge1'].fmeasure)\n",
        "\n",
        "plot_data = pd.DataFrame({\n",
        "    'Transcript Length': transcript_lengths,\n",
        "    'ROUGE-1 Score': instance_rouge_scores\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(\n",
        "    data=plot_data,\n",
        "    x='Transcript Length',\n",
        "    y='ROUGE-1 Score',\n",
        "    scatter_kws={'alpha': 0.5},\n",
        "    line_kws={'color': 'red'}\n",
        ")\n",
        "plt.title('ROUGE-1 Score vs Transcript Length')\n",
        "plt.xlabel('Transcript Length (words)')\n",
        "plt.ylabel('ROUGE-1 F1 Score')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.savefig('visual_rouge_vs_length.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 7.2 Visual 2: Lexical vs. Semantic Gap\n",
        "df_comparison = pd.DataFrame({\n",
        "    'Metric Type': ['ROUGE-1 (Lexical)'] * len(instance_rouge_scores) + ['Cosine Similarity (Semantic)'] * len(cosine_scores),\n",
        "    'Score': instance_rouge_scores + cosine_scores\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='Metric Type', y='Score', data=df_comparison, palette=\"muted\", inner=\"quartile\")\n",
        "plt.title('Validation: Lexical Overlap vs. Semantic Understanding')\n",
        "plt.ylabel('Score (0.0 - 1.0)')\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotation for the gap\n",
        "avg_gap = np.mean(cosine_scores) - np.mean(instance_rouge_scores)\n",
        "plt.text(0.5, 0.9, f\"Avg Gap: +{avg_gap:.2f}\",\n",
        "         transform=plt.gca().transAxes, ha='center', fontsize=12,\n",
        "         bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
        "plt.savefig('visual_semantic_vs_lexical.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T_ilsKeScKNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Model Packaging\n",
        "\n",
        "Compressing the model artifacts for distribution or deployment."
      ],
      "metadata": {
        "id": "FOayrq-FcN3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "print(\"[INFO] Zipping model artifacts...\")\n",
        "shutil.make_archive('final_model_package', 'zip', 'final_flan_t5_model')\n",
        "print(\"Done. You can download 'final_model_package.zip' from the files panel.\")"
      ],
      "metadata": {
        "id": "3qPEX6nrcO8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}