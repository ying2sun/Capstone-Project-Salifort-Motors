{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eea254ecf1df44d68e46217e9f5af952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40858dae0b3841838c2fe5c8b6809a0c",
              "IPY_MODEL_821b5d974b6444af9f82e9f51b0a0e33",
              "IPY_MODEL_0dbb1feaafd9477396fc93621c878d6f"
            ],
            "layout": "IPY_MODEL_0e4a184a892f44b9b1ace21a6fc5494d"
          }
        },
        "40858dae0b3841838c2fe5c8b6809a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d42502afce742cc886a20609dcbf2c1",
            "placeholder": "​",
            "style": "IPY_MODEL_ab6c05b420284df589d269f2f0e38964",
            "value": "Batches: 100%"
          }
        },
        "821b5d974b6444af9f82e9f51b0a0e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fdfc417000a402e913e1a31f5d1f10a",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63be40ecba7d4a6aac066c902380846d",
            "value": 23
          }
        },
        "0dbb1feaafd9477396fc93621c878d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89fcd453f93e42e9a5af8f413b3260a8",
            "placeholder": "​",
            "style": "IPY_MODEL_519d20ffa9ff479ba275ee8bece09e8c",
            "value": " 23/23 [00:45&lt;00:00,  1.76s/it]"
          }
        },
        "0e4a184a892f44b9b1ace21a6fc5494d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d42502afce742cc886a20609dcbf2c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6c05b420284df589d269f2f0e38964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fdfc417000a402e913e1a31f5d1f10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63be40ecba7d4a6aac066c902380846d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89fcd453f93e42e9a5af8f413b3260a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519d20ffa9ff479ba275ee8bece09e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8978c959635749b1974c7b93c411b210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d513277ff254dbaa936d2d4ab40fd24",
              "IPY_MODEL_f0c6d940bdce4c62b9a2d71a298c0a22",
              "IPY_MODEL_b5524cc9a1bf45378ba4a56d58737cb0"
            ],
            "layout": "IPY_MODEL_aec8f6342ed243308fb98f0512792f55"
          }
        },
        "4d513277ff254dbaa936d2d4ab40fd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4676029fb754f25868260e5fcfd6bea",
            "placeholder": "​",
            "style": "IPY_MODEL_f5324a2b7a134c8cb1de518f82d9749c",
            "value": "Batches: 100%"
          }
        },
        "f0c6d940bdce4c62b9a2d71a298c0a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f11240d1b90e4996b2eb69c14b23208e",
            "max": 91,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08327669f3dc419fb884ca0b1eac4296",
            "value": 91
          }
        },
        "b5524cc9a1bf45378ba4a56d58737cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56713b5aada43638189c98e89f8a262",
            "placeholder": "​",
            "style": "IPY_MODEL_33169022953e46bab1dc68046ce3991b",
            "value": " 91/91 [01:13&lt;00:00,  1.73it/s]"
          }
        },
        "aec8f6342ed243308fb98f0512792f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4676029fb754f25868260e5fcfd6bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5324a2b7a134c8cb1de518f82d9749c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f11240d1b90e4996b2eb69c14b23208e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08327669f3dc419fb884ca0b1eac4296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e56713b5aada43638189c98e89f8a262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33169022953e46bab1dc68046ce3991b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ying2sun/Capstone-Project-Salifort-Motors/blob/main/Youtube_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1: Load Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "BCq5E7jWCRK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Required Packages"
      ],
      "metadata": {
        "id": "gPs8SLam_R5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeelruG8-FeG",
        "outputId": "f4e2cb4d-b419-44f8-b503-4e428185f022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu datasets transformers tqdm nltk pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import Libraries"
      ],
      "metadata": {
        "id": "zox_R8tu-vQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ],
      "metadata": {
        "id": "x9oQ0OVH-hHh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Download NLTK Punkt Tokenizer"
      ],
      "metadata": {
        "id": "vL0nJ1lD-zZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg0B9N2O-k_a",
        "outputId": "2605a0e3-4b38-46ff-f51e-9a1c9f9aa167"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cb3oSzzAMqZ",
        "outputId": "5b3a36ac-c426-4368-e7ef-0ec5edada2de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Utility Functions"
      ],
      "metadata": {
        "id": "6q7femDv-4a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_sentences(group):\n",
        "    # Concatenate all sentences for one video\n",
        "    return ' '.join(group['text'])\n",
        "\n",
        "def get_first_n_sentences(text, n):\n",
        "    # Get the first n sentences from a transcript string\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    sentences = ' '.join(sentences[:n])\n",
        "    if len(sentences.split()) > 1000:\n",
        "      sentences = ' '.join(sentences.split()[:300])\n",
        "    return sentences\n",
        "\n",
        "def get_summary(row, pipe, first_n_sent=20):\n",
        "    # Generate a summary from the transcript\n",
        "    subset = get_first_n_sentences(row['full_transcript'], first_n_sent)\n",
        "    result = pipe(subset, max_length=50, min_length=10, do_sample=False, truncation=True)[0]['summary_text']\n",
        "    return {\"summary\": result}\n",
        "\n",
        "def get_embeddings(row, model, first_n_sent=10):\n",
        "    # Get sentence embedding for transcript and summary\n",
        "    transcript = get_first_n_sentences(row['full_transcript'], first_n_sent)\n",
        "    summary = row['summary']\n",
        "    transcript_embedding = model.encode(transcript)\n",
        "    summary_embedding = model.encode(summary)\n",
        "    return {\n",
        "        'transcript_embedding': transcript_embedding,\n",
        "        'summary_embedding': summary_embedding\n",
        "    }"
      ],
      "metadata": {
        "id": "s_C7T2Xu-plF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Load and Process Dataset"
      ],
      "metadata": {
        "id": "AHzAfAtf-9gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path='jamescalam/youtube-transcriptions', split='train', n_rows=10, summarize_first_n_sents=20,video_ids = []):\n",
        "    # Load dataset from HuggingFace\n",
        "    data = load_dataset(path, split=split)\n",
        "    df = pd.DataFrame(data)\n",
        "    group_cols = ['title', 'published', 'url', 'video_id', 'channel_id']\n",
        "    # Group by video, concatenate transcript\n",
        "    results = []\n",
        "    for name, group in tqdm(df.groupby(group_cols)):\n",
        "        full_trans = concat_sentences(group)\n",
        "        row = {col: name[idx] for idx, col in enumerate(group_cols)}\n",
        "        row['full_transcript'] = full_trans\n",
        "        results.append(row)\n",
        "    df_agg = pd.DataFrame(results)\n",
        "    if n_rows > 0:\n",
        "        df_agg = df_agg.head(n_rows)\n",
        "    elif len(video_ids) > 0:\n",
        "        df_agg = df_agg[df_agg['video_id'].apply(lambda x: x in video_ids)]\n",
        "    df_agg.dropna(subset=['full_transcript'], inplace=True)\n",
        "    return df_agg\n"
      ],
      "metadata": {
        "id": "CTHuz8Vh_Bcv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Main Processing Block"
      ],
      "metadata": {
        "id": "-TiFKQcs_GU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = load_data(n_rows=10, summarize_first_n_sents=20)\n",
        "\n",
        "# Create summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Generate summaries\n",
        "df['summary'] = df.apply(lambda row: get_summary(row, summarizer, first_n_sent=20)['summary'], axis=1)\n",
        "\n",
        "# Generate embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "df['transcript_embedding'] = df.apply(lambda row: model.encode(get_first_n_sentences(row['full_transcript'], 10)), axis=1)\n",
        "df['summary_embedding'] = df.apply(lambda row: model.encode(row['summary']), axis=1)\n",
        "\n",
        "# Now df contains all fields we need for downstream RAG/retrieval/analysis\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "zLI9STTh_JNI",
        "outputId": "67f41e36-8746-4225-a197-b5bd3d3f697c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "100%|██████████| 701/701 [00:00<00:00, 1882.96it/s]\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title            published  \\\n",
              "0                             $5 MILLION AI for FREE  2022-08-12 15:18:07   \n",
              "1  1 week out from SPARTAN Race + How to bake a s...  2018-04-26 07:36:40   \n",
              "2  10-minute Bodyweight Back &amp; Shoulders Bedr...  2020-03-25 08:59:18   \n",
              "3  10-minute Lower Body Bodyweight Workout for Be...  2020-04-03 07:09:41   \n",
              "4  10-minute Morning Wake Up Stretching Routine f...  2020-03-30 08:47:10   \n",
              "\n",
              "                            url     video_id                channel_id  \\\n",
              "0  https://youtu.be/3EjtHs_lXnk  3EjtHs_lXnk  UCfzlCWGWYyIQ0aLC5w48gBQ   \n",
              "1  https://youtu.be/04HX2zgQNXE  04HX2zgQNXE  UCr8O8l5cCX85Oem1d18EezQ   \n",
              "2  https://youtu.be/SBLp0Z4pJko  SBLp0Z4pJko  UCr8O8l5cCX85Oem1d18EezQ   \n",
              "3  https://youtu.be/HtSeYhm1e7A  HtSeYhm1e7A  UCr8O8l5cCX85Oem1d18EezQ   \n",
              "4  https://youtu.be/wm5QkbQ5LFI  wm5QkbQ5LFI  UCr8O8l5cCX85Oem1d18EezQ   \n",
              "\n",
              "                                     full_transcript  \\\n",
              "0  Imagine an AI where all in the same model you ...   \n",
              "1  What's going on guys welcome back to another S...   \n",
              "2  You know in the coming months, leadership at e...   \n",
              "3  Oh, I'm building this hat. Look how good it is...   \n",
              "4  Good morning! We are up to day 14 of Reps For ...   \n",
              "\n",
              "                                             summary  \\\n",
              "0  A group of over a thousand researchers has bee...   \n",
              "1  Spartan trainer is back with a grip and stabil...   \n",
              "2  Leadership at every level is going to be like ...   \n",
              "3  Day 18 of streamlit.io's 10-minute workout ser...   \n",
              "4  We are up to day 14 of Reps For Rona. Today on...   \n",
              "\n",
              "                                transcript_embedding  \\\n",
              "0  [-0.07433385, -0.13800655, -0.04107823, 0.0329...   \n",
              "1  [-0.062081713, -0.05884585, 0.0019795096, -0.0...   \n",
              "2  [-0.003295755, 0.05800058, 0.011247904, -0.038...   \n",
              "3  [-0.011118657, -0.014251923, 0.0387789, 0.0440...   \n",
              "4  [-0.0476916, -0.0415211, 0.030855495, 0.031183...   \n",
              "\n",
              "                                   summary_embedding  \n",
              "0  [-0.040884223, -0.10034384, -0.05008944, 0.048...  \n",
              "1  [-0.08060527, -0.04789059, -0.0043220487, -0.0...  \n",
              "2  [-0.09108453, -0.034012754, 0.0183743, -0.0607...  \n",
              "3  [-0.0782866, 0.054833915, 0.013783748, -0.0164...  \n",
              "4  [-0.0937326, -0.009859586, 0.01704017, 0.01686...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3faa020-9da0-4b18-93b6-00d96dedd3b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>full_transcript</th>\n",
              "      <th>summary</th>\n",
              "      <th>transcript_embedding</th>\n",
              "      <th>summary_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$5 MILLION AI for FREE</td>\n",
              "      <td>2022-08-12 15:18:07</td>\n",
              "      <td>https://youtu.be/3EjtHs_lXnk</td>\n",
              "      <td>3EjtHs_lXnk</td>\n",
              "      <td>UCfzlCWGWYyIQ0aLC5w48gBQ</td>\n",
              "      <td>Imagine an AI where all in the same model you ...</td>\n",
              "      <td>A group of over a thousand researchers has bee...</td>\n",
              "      <td>[-0.07433385, -0.13800655, -0.04107823, 0.0329...</td>\n",
              "      <td>[-0.040884223, -0.10034384, -0.05008944, 0.048...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 week out from SPARTAN Race + How to bake a s...</td>\n",
              "      <td>2018-04-26 07:36:40</td>\n",
              "      <td>https://youtu.be/04HX2zgQNXE</td>\n",
              "      <td>04HX2zgQNXE</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>What's going on guys welcome back to another S...</td>\n",
              "      <td>Spartan trainer is back with a grip and stabil...</td>\n",
              "      <td>[-0.062081713, -0.05884585, 0.0019795096, -0.0...</td>\n",
              "      <td>[-0.08060527, -0.04789059, -0.0043220487, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10-minute Bodyweight Back &amp;amp; Shoulders Bedr...</td>\n",
              "      <td>2020-03-25 08:59:18</td>\n",
              "      <td>https://youtu.be/SBLp0Z4pJko</td>\n",
              "      <td>SBLp0Z4pJko</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>You know in the coming months, leadership at e...</td>\n",
              "      <td>Leadership at every level is going to be like ...</td>\n",
              "      <td>[-0.003295755, 0.05800058, 0.011247904, -0.038...</td>\n",
              "      <td>[-0.09108453, -0.034012754, 0.0183743, -0.0607...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10-minute Lower Body Bodyweight Workout for Be...</td>\n",
              "      <td>2020-04-03 07:09:41</td>\n",
              "      <td>https://youtu.be/HtSeYhm1e7A</td>\n",
              "      <td>HtSeYhm1e7A</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>Oh, I'm building this hat. Look how good it is...</td>\n",
              "      <td>Day 18 of streamlit.io's 10-minute workout ser...</td>\n",
              "      <td>[-0.011118657, -0.014251923, 0.0387789, 0.0440...</td>\n",
              "      <td>[-0.0782866, 0.054833915, 0.013783748, -0.0164...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10-minute Morning Wake Up Stretching Routine f...</td>\n",
              "      <td>2020-03-30 08:47:10</td>\n",
              "      <td>https://youtu.be/wm5QkbQ5LFI</td>\n",
              "      <td>wm5QkbQ5LFI</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>Good morning! We are up to day 14 of Reps For ...</td>\n",
              "      <td>We are up to day 14 of Reps For Rona. Today on...</td>\n",
              "      <td>[-0.0476916, -0.0415211, 0.030855495, 0.031183...</td>\n",
              "      <td>[-0.0937326, -0.009859586, 0.01704017, 0.01686...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3faa020-9da0-4b18-93b6-00d96dedd3b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3faa020-9da0-4b18-93b6-00d96dedd3b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3faa020-9da0-4b18-93b6-00d96dedd3b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-49afc318-08ce-41df-98ac-8b7bc9034a30\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49afc318-08ce-41df-98ac-8b7bc9034a30')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-49afc318-08ce-41df-98ac-8b7bc9034a30 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2M All-In into $5 Pot! WWYD? Daniel Negreanu&#39;s No-Limit Hold&#39;em Challenge! (Poker Hand Analysis)\",\n          \"1 week out from SPARTAN Race + How to bake a sweet potato in 5-minutes!\",\n          \"100K Subs AMA (Ask Me Anything)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2020-12-13 18:15:43\",\n          \"2018-04-26 07:36:40\",\n          \"2021-09-26 17:50:18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"https://youtu.be/R07CVhWbAXc\",\n          \"https://youtu.be/04HX2zgQNXE\",\n          \"https://youtu.be/tfYm4zcAnW0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"R07CVhWbAXc\",\n          \"04HX2zgQNXE\",\n          \"tfYm4zcAnW0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"UCfzlCWGWYyIQ0aLC5w48gBQ\",\n          \"UCr8O8l5cCX85Oem1d18EezQ\",\n          \"UCZHmQk67mSJgfCCTn7xBfew\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Hi there, today I want to bring to you a little bit of a different video. The video right now is supposed to be sort of a motivational lead up to the next video I want to release. And the next video is going to be about Facebook's new rebel algorithm, which is an algorithm that solves two player zero sum imperfect information games. So it is very similar in to the Alpha Zero algorithm or the AlphaGo algorithm, just that line of algorithms that combine search and learning. But whereas the alpha line is in perfect information games, so games where you can see everything like chess or go, the rebel algorithm is an imperfect information games. And one example of this is poker. So heads up, heads up poker, like heads up, Texas Hold'em, no limit, let's say in this case, is a two player zero sum. Let's assume the house doesn't take a rake. Two player zero sum imperfect information game, which this algorithm rebel can solve better than apparently anything before it. And Daniel Ngranu, who is a, you know, a longtime poker pro has released these polls on Twitter, which I found just to be very interesting. So the timing was very fitting. And I thought I sort of make a lead up video to the next paper video, just to sort of get you into the thinking if you've never if you've never played poker at sort of beyond an amateur level, I sort of want to motivate you what makes this game so interesting, because it seems pretty simple at the start. Okay, so here we go. The Daniel Ngranu poses the following question, poker question for you all. And maybe I should briefly explain how the game works for anyone who doesn't know there. And if you have one minute, if you know, just jump ahead one minute or so. So at the beginning, you get two cards, your opponent gets two cards, you don't know the opponent's cards, the opponent doesn't know your cards, then success successively on the board, they're going to be revealed first three cards at a time, which is called the flop. Then there's one other card, which is called the turn. And then there's another card, which is called the river. And there are four betting rounds. So there's one betting round pre flop, which is when no cards are on the table, there's one betting round at the flop, one at the turn and one at the river. And then if the players are still in and haven't folded, the cards are revealed and scored according to the normal rules of poker. So your two cards and the table five cards, you get to choose any five of those seven to make up the poker hand, whoever has the better poker hand wins. Okay. So in this situation here, you have aces, so your whole cards are two aces, which is you know, the best pre flop hand, but the board is ace, aka eight, four, four, so ace king, eight, four, and four. So that's the board, which gives you a full house aces with fours, okay, which is the second best hand that's possible on this board. So you have the second best hand, usually you would be happy to put all your money in into this board. Because the only hand that's better than you is if your opponent has two fours. So that is a possibility, right? But it's a very, very, very slim possibility. So you might think I want to put all my money into here. But now, you know, now comes the tricky part is you put all your money in here, because you say, well, there's only really one hand that beats me, okay, but you have to think ahead and say, how often does my opponent have that hand? And crucially, crucially, how often are they going to give me their money while not having this hand? So let's say your opponent has an eight and a nine, okay. And, and so they have a pair of eights, which you know, they might think, you know, I have a pair pairs, okay. But you put in a lot of money, they're probably going to fold that hand, right? So if you put in a lot of money here, they're not giving you any money. So if now, let's say they have like, two kings, which is a very strong hand on this board. But if you put in like, exorbitant amounts of money, still, they're going to conclude, well, it's, it's not worth it, like, there are still better hands I'm going to fold. So all of this, it's not just a question of which cards do you have? It's not even a question which cards your opponent has, it's, it's a it's a question also of how much money do you put in? Because that regulates very much how the strategies are, I hope I hope you can sort of see that. So you always have to think about what possible cards could my opponents hold? And which of these cards are they willing to put in how much money into the pot? And then from that, you can determine, is that profitable for me or not? In this particular situation, there are $5 already in the pot. So all the previous betting rounds, they get collected into what's called the pot. So the pot here, in this case is $5. And your opponent, your opponent bets $2 million, okay, so $2 million on the pot into a pot of five, it's obviously a constructed scenario, but your opponent now puts up 2 million, okay, so you have to put in 2 million into a pot that's now $2 million and $5. So if you let's say if you fold, you lose whatever you put in of these $5. So you shouldn't think that sunk cost anyway, you should simply think I put in 2 million in order to win five plus the 2 million the opponent puts in, okay. So obviously, this is exactly the reverse of what we looked at now your opponent is putting in a ginormous amount of money, okay, and you, you have the second best hand. So this this get now gets interesting. Now there is an additional complication here, would you call or fold against the guy who always goes in on the river every hand, okay, this is an additional information, somehow, you know, that this person always goes in on the river. So on the river, they always shove all their money all in. That's what you know. Now, a lot of people would lean to an easy call here, a lot of people would say, of course, they're going to all in with any like any, anytime they're on the river. So of course, I'm going to call it the second best hand, there are many, many hands and if they're going to do this with all hands, but that's not the case. They're just because they always go all in on the river every hand. I think this is slightly under specified. It's every hand where they get to the river, right? So here, a smart opponent, let's say this is a smart opponent. But for some reason, someone kidnapped their dog and threatens to kill the dog if they don't always go all in on the river. But other than that, they're very smart player. So they, they now also know that they always go all in on the river, because you know, they always go in all in on the river. So what they will do is, once they're on the flop and the turn, they will only ever continue with hands where they would go all in all in on the river, right? And they they not only they not don't always have 2 million in the end on the table, they might have no smaller values. So when they are on the flop, and when they are on the turn, they are very much aware that they have this giant amount of money, and that they must go all in if they reach the river. So conceivably, they would fold every hand that they weren't willing to go all in on the river. So they they won't have just any cards they that that seriously skews their distribution of cards that they could hold because they make that inference, right? So now you can sit here and say, Okay, it's conceivable that they actually hold off on, you know, most of their cards, they would fold most of their cards on the on the flop or turn, given that they must always go all in all in on the river. So let's actually look at the turn. So let's imagine we do not know that this is a four, right? So we the last decisions are made here, right here, when it's the when it's the turn. Here, your opponent will only go to the river with cards where they feel that they can then fully go all in all the way, right? That's because they also know they go all in every time they reach the river. So the question is, what possible range could they do this with? And one possibility is like they they do it. If they know they have 2 million it's a very risky move to go all in on the river, right? So conceivably, I'd say they would not do it with two fours because they can't possibly know that another four is coming the chances so incredibly slim. However, of course, that strategy now also changes the range of hands that you continue to the river with so you can be you knowing that the opponent will only go to the river with cards where they could go all in on the river also will change your distribution. But just in this particular situation, I would say the following. If this is the case, the opponent can't possibly know that there's another four coming. Therefore, their range here, if it includes two fours, if it includes those, it will also include something like two kings, it will also include something like ace four, or king four, like conceivably because those maybe not but two eights maybe. But at least two kings, so their range is conceivably yeah, if it includes two fours, it must include two eights and two kings, right? Because these are strictly better at the turn. It could even be any ace because that blocks you from having an ace. So if they can have fours at the end, they can also have kings and eights. And just because they can have those hands, it probably makes for a for a good call here on the river because you are beating kings and eights on on the river. Specifically, the fours are much more unlikely because the four is actually in the deck since we we already know it's coming right here. So in this case, I would call because of those whole reasoning not because I have the second best hand, right, I hope you can sort of see how this back and forth goes. So you assume that your opponent is smart, your opponent assumes that you are smart. And then you sort of reason 123 levels in depth. And of course, if you reason to infinity, that becomes a Nash equilibrium. And that's exactly what this rebel algorithm approximates. I would have guessed that this situation is much more interesting if you reverse the board. So if the board was something like 448444, four, four, ace, King eight or something like this, where your opponent clearly already has the best possible hand before they enter the river, that would make would would make it quite a bit more interesting, I believe. And I, I don't know what the analysis would be. But let's go on to the next 10. So that would be my guess would be called. I haven't, as you can see, I haven't answered yet I will after the video. But it's irrelevant because the most comments I read are just like inferring very simple things, which are, as I say irrelevant. So the follow up question here is their same situation $5 in the pot to me, the opponent bets 2 million all in on the river board is the same you have aces, would you call a fold against the player you know nothing about? Okay, so here's a player you know nothing about. Now, the you know nothing about is so now you like, now you have to estimate probabilities that the person is brain dead and things like this, right? But what you can do, what you can do is always just estimate sort of the Nash equilibrium strategy of the situation and maybe go with that because at least then you cannot lose an expectation. So if you fact if you like factor in the fact that the person might be dumb or brain dead or something like this, then if you mess up these probabilities, you are in fact exploitable. Though, you know, the exploitability only matters if that situation happens over and over and over and over again, whereas I think this is going to happen to you at maximum once. However, same situation, but your opponent does not go all in on the river every hand, you know nothing about them, right? The board happens as it is. And all of a sudden, this person pushes 2 million. Now let's analyze this. So you might think, hey, this person pushes 2 million in the pot of $5. They must hold the nuts very, very, very, very often for this to be profitable, right? So they probably hold the two fours right here. But then again, if you infer that you might want to go ahead and fold those aces, okay, you fold the aces. So your opponent thinks about this, and they realize, wait a minute, if I can get them to fold aces, which is the second best hand on this board, right? I should probably push this much money a lot more often, because I can, you know, like I can get them off aces, I can probably get them off most hands that they are in this situation with right on this board, a ace, King eight, we don't know the colors, but there are a lot of hands that get to the river in this situation. So I can bluff them off a lot of them by simply pushing 2 million in the pot, right? But then it's this old game, you push 2 million to win $5. This has to work very often. In fact, this has to work now it has to work like 4,399,000 out of 400,000 times to break even right? If it if it doesn't work even one time. Yeah, so if you're up, if you fold anything, but the but the absolute nuts, your opponent might actually just hold a single four, because then they know you don't have two fours. And then they know you can't possibly have the best hand, then it can push you off of it. But then, right, they, if they bluff a certain amount of time, if they don't need to bluff often for you to actually make it profitable. And if they do, in fact bluff. So let's let's assume they just bluff if they have a four, because they know you can't have both fours because they have one. So you can never have the best hand. And they think if they bet 2 million, they can push you off any hand. Now you go ahead and you say, wait a minute, if they bluff whenever they have a single four, they're much more often going to have a single four, like maybe they have a four, four, nine or something like this, they're much more often going to have a hand like this, then two fours just combinatorically, right? So maybe they're actually on a bluff pretty often here if they do this every single time they have a four. So I can actually call, it doesn't even matter that I have aces, right, I can call with any any hand that hits anything on this board is probably going to beat though, if they have a four, they have trips. So let's say if they bluff with any hand, I can call with any hand. And they will think about this and say, oh, maybe I shouldn't bluff with any hand, right? I should probably moderate that because the other person will adjust. If they bluff with a four, they have trip fours. So I even if they bluff with a four, I might only and it is a bluff. Like if you have a four and you bet 2 million here, that's a bluff. Like you're clearly trying to get someone off of like aces. Because it's not like you don't bet for value 2 million into $5 with this. So I will only call with aces, kings, eights, ace four, king four, eight four, stuff like this because they all beat a single four, right? And now the question becomes, again, how so there is there is the number of hands I will call with like aces, kings, and so on. Ace four, how these are a subset of hands, or maybe not like this as subset of hands, probably a large subset of all the hands that I would hold on the river like that I would get to the river with right here. And they are going to push me off of those hands with with any large bet. But this this bet is really meant to get me off of those strong hands. So the question is, how often do they do this with a four in order to still be profitable? So we get back to this sort of inference of how often can this be a bluff for me to legitimately call here? And that factors in how often I am on the river and how often on the river I hold one of these hands that I could conceivably catch a bluff with. So you can see that a lot of a lot of stuff is going in here. Me personally, I would say that I know nothing about this person, I would probably fold in this in this case. Because if I assume they're smart, they must know that they can only pull this 2 million into $5 thing very, very few times if they don't have the absolute nuts in this case. And if they don't have the nuts, it almost it almost doesn't matter what they have, they probably have a single four and then yeah, the number of hands that I can have on the river that are going to catch a bluff with a single four is just too large for them to often bluff right here. Of course, if we both play if if the person plays Nash optimal, then I have like some assignment to call or fold right probability of call probability of fold that I would do in this particular situation. And and it's going to be break even. Okay, last question, though that might not be true, I might have actually a fixed binary decision here. No, because that influences their strategy to Yeah. Last question, same thing. But now, which hand would be better to have if you choose to call, so you, you choose to call, but now, which hand would you rather have in that situation? Would you have king four or aces? So some people might say, well, aces, clearly because aces here is the better hand than King four, right? aces is full house aces full of fours and King four is fours full of kings. So let's say you imagine you have King four, why would you want to have King four, you would want to have King four, because now your opponent can't have two fours anymore. Okay, so the possibility of your opponent holding two force is off the table because there are only four fours in the deck. And the so you're blocking that possibility that your opponent has two force. So they cannot have the nuts possibly. They, it's much more probable now that in fact, they have a single four, right? And they are trying to push you off of something like aces. You see, so it's a bit the same situation as before. And we can we can remark that King four is also in this hands that we would call with. But so are the aces. Now, it all again boils down to what's the frequency of them folding here. And that boils down to what's the proportion of hands that you have here plus what's the frequency of them that you call with? So the question is, would you rather have aces or King four? And why would you why would you rather have aces? What would be reasons that you would rather have aces? Well, if your opponent is smart, they might think that and I haven't thought this through before, but let's just try to figure this out together. Your opponent. So if you'd rather have aces than King four, that must mean that your opponent would do this conceivably with hands that you beat with aces, but not with King four, like you, you decide to call that's a given you decide to call. So now everyone reveals their cards. And so if you say you'd rather have aces, that means you think that your opponent would do this kind of stuff with something like that Kings or eights or something like this, something that would beat King four, but not beat aces. So your opponent, your opponent might be smart and think, Wait a minute. If this person has an a four, right, then they will think that I cannot possibly have two fours. And therefore they will call with a single four, even if I bet 2 million date, they will think, who I have the four and therefore they can't have the four. So this must be one of those rare times where they bluff, right. And, and then they might say, Well, but I have two eights, right, I have two eights, I beat a single four. And therefore, I can actually get money out of anyone that's trying to catch my bluff, because they have a single four. So now the question is, how often does anyone on the river here have a single four? And again, this is where I go and say that board would be probably more interesting if it was this way around, because it's much more conceivable that anyone has a single four lying around. If the flop was this already, though, King four conceivably is you hit the king on the flop, and then you somehow get through to the river while scoring two fours, but it's just not as likely that you still have the four around. And so, again, this is where I still have the four around. But still, you can sort of see the thinking right. So the opponent might think, wait, they're going to call me with any old four, especially with like, also with like King four, I have eights, I beat things like Ace four, King four, I beat a single four, my opponent's gonna think I only do the 2 million things with two fours, my opponent's gonna have a four, they will infer that I can't have a four, they will call me because they think I'm bluffing and ta da da da. Okay, so you can see that it goes pretty, pretty deep. And then in that case, they will push with the eights. And in that case, you much rather have the aces right here, because they don't know whether you have the four or not, right. But if you have the aces, again, you do not have the four, and it is very possible that your opponent has two fours. And after all, it's 2 million into a pot of $5, they would only, they have to have a very good hand very often for this to be profitable. Okay, so this, this kind of thinking is, is what computation of an Ash equilibrium, in effect boils down to. So we're going to see, I don't know what the correct answers to this is, by the way, I the even the rebel source code isn't open source for poker, the code is open source, but the implementation for poker isn't and I think the checkpoints for poker aren't. So maybe we won't we won't find out, I would love to hear your opinions on this, maybe I am completely wrong right here. But this is about what an algorithm like that has has to do. And I hope I've sort of given you an overview of why this sort of games are interesting, what these algorithms need to think about, and why it is so much harder than something like chess or go not that the game itself is harder, but you have to constantly reason about things that you do not know. And you constantly have to assign probabilities and combinatorial fractioning. How often does this happen? How often does this happen? And then you have to adjust each time when you adjust your strategy, you have to think that your opponent can make the same conclusions, given the observed state, and they can also adjust their strategy. So that's the difficulty. Those are the questions I would say you go vote, see what other people have to say. And maybe Daniel will let us know once the polls are over. Alright, so that was it for me. Thanks a lot for watching. \",\n          \"What's going on guys welcome back to another Spartan trainer and as of today Officially one week out so this is I think one of the last three or last four training sessions that I'm doing It's a grip and stability workout. You probably noticed I'm sitting on the ground right now. I don't know I'm in a bit of warm-up position. I'll show you how to do it It's one of my favorite warm-up exercises. So what you want to do is get your butt next to the wall I start sidewards and put my legs I rotate them up against the wall and then I kind of move my butt closer to the wall and Put my legs into like a squat position And now the idea is get your butt right up press against the wall as close as you can It's going to be really tight on your hips and and your glutes and your ankles and your knees But it's it's a really great stretch to get your lower body Primed for the exercises you're about to do in the gym and it can be can be for any workout Because I mean your hips come into play with everything and you can also set up bending your knees put your legs Straight up against the wall and spread them out like kind of in a in a split position And so I alternate between both these I'll try to hold them for about a minute But what do we have on today's workout agenda? Well, it is a strength and stability workout or grip and stability workout So first exercise we're going to start off I'm going to start off with a circuit and we're also going to do two circuits today Actually, it's ten burpees five pull-ups one minute plank one minute plate carry one minute rest We do five rounds of that and then we get into ten deadlifts some beast activations Which is like a core exercise and then some hanging leg raises and we'll do a few more rounds of that I think that's a ten minute arm wrap. Let's jump in. All right, so we ran into a little bit of a roadblock This is dead, but not to worry. We've got my trusty pal Benny. What's going on, brother? We got the I got the iPhone cam. So So yeah, that's it. No excuses. I'm gonna get this workout filmed start off first thing first burpees Nothing will get your heart rate up like burpees We got five pull-ups One minute plank Attention to the next dimension Let's just let our bodies be true. We make out but we love it mess around for the thrill of it One minute plank done. We've got one minute plate carry. I picked the picket I picked the thicker plate because it's a bit harder to grip At least one minute hint Lets rock Okay, so that's one round. We're gonna do that five times usually with the plate carry. That must have been a fast walk. It's about a minute all the way to the end of the gym and back. So four rounds to go. Five sets done. That's only like 15 minutes of work, but it's ridiculous. We're gonna go into a 10 minute AMRAP. Got deadlifts, beast activations, which are a core exercise, and then some hanging leg raise. More core exercise. Five hanging leg raises. And then five reps of beast activation, both sides. This is not the exact beast activation in the catalog, but it works for me. Now the deadlift isn't as heavy as what I've usually been going, but that's because we're in the later stages of training and I want to keep my legs fresh for a bunch of running. So about 15 seconds rest and we'll just keep going, repeating it over and over. 10 minute cycle. So the goal of that workout is to get the grip strength going so when you're on the obstacles, you can hold on and move around. Same with the core strength. Over those long distances, your legs could probably take it, but your core is what'll end up letting you down. So that was the main focus of this workout. And of course the short rest times to mimic in between the running and the obstacles. Let's go eat. We're going to bake a sweet potato in five minutes. Okay, well this hotter than the sun type object is cooling. We've got another one in the microwave going, but while that's running, we're going to prep the other stuff so by the time this is out, everything's ready to go. My A wise man once told me to never eat alone. Alright, yeah so I'm gonna, what I'm gonna do, I'm gonna go, how is it mate? And then I'm gonna sign off the video that I did. Alright, so you say, when I ask you how is it mate, just say whatever you think and then I'll put the finishing touches on with a bit of lemon. Then I'll sign out the video. Alright, so how's it look mate? Mate, I can't believe what I've got in front of me. It is a wish bang new beauty. We've got one more thing, the finishing touch. We're gonna squeeze a lemon on there to finish off. Yeah. I'll do the same on mine. So this is going on YouTube dad, you know what that is? Yeah, not. I'm one week out from Spartan Race, how do you reckon I'll go? Spartans were pretty good, you're not Greek, that's the only problem. Alright, I won't hold you too long on this. So we've got some greens at the bottom, sweet potato, avocado, some hokey fish, some spices, a little bit of capsicum and of course topped it off with a drizzle of olive oil and lemon to finish off. You keen to eat? I'm very keen to eat. I'm holding back here, I have to say. Don't, don't, don't, don't, I need to get the video. One week out from Spartan Race, I'm getting keen, I'm a little bit scared but we'll get it done. Nutrition's on point, training's going well, we'll see you next week. You wanna sign off with me? Yeah. \",\n          \"Howdy-dowdy. Can anyone confirm that you can see me? That'd be nice. Excellent. Cool. Yeah hi everyone. I just announced that there would be a ask me anything for the 100k and I stare blank green screen. Sorry. What do you want me to do? Yeah so I thought this would be a neat thing to do and sorry it's a bit spontaneous but I had time and I realized I wouldn't have time. So much so many more times. Yeah thanks everyone for subscribing. Thanks everyone for being here and there's not really a plan for now so we'll just see maybe this is over after five minutes or so but in general it's ask me anything. It is not I'll answer anything but it's not topical. So I'm trying to catch up. Will there be more live coding sessions in the future? I hope so. I'm not really sure. I get less competent as I age with respect to coding. But yeah I don't know if live coding for some things it's good if you know what you're doing and if the experiments don't need a lot of time but inherently in machine learning at some point you do train and then you wait and then what do you do with the live stream. So I don't know if someone has neat ideas of how to bridge the time there. Yeah maybe projects where you can do first a chunk and then let it train and then the next day come back or something like this. I don't know. Dimitros asks do we need a PhD in AI to get into industry? No certainly not. It might be a little bit more difficult to get into a research position given that many people are doing AI PhDs now but also companies are looking for them. But what's also always an option is that you start as a software engineer in a company and then just you essentially make friends with the research team and you just show them that you're quite competent at research. You know however you get competent once they know you are competent they'll probably want you on their team. So if there is head count that's another way to get into the research in industry. Did you know Arsenal scored three? No sorry. Why is there no Minecraft background? I'll try to get one going. Your PhD thesis where are you working or planning to work? I work my PhD thesis is done. I do officially have my title. I'm planning I'm working in Zurich. I have a startup in NLP so yay. Most problems in deep learning boil down to cross entropy loss and gradient descent. This realization is very demotivating due to the lack of intellectual stimulus insights. Well yeah I mean you know most problems in networking boil down to sending things over cables. Yeah it's true that at least the current paradigm is deep learning which means it's gradient descent in large models but there is still enormous creative freedom on how you design the models how exactly you use them or even to just sort of research what happens when you do these very simple principles right. It's astounding very simple principles very simple training methods paired with what is essentially quite simple blocks of computation give this incredible result and I think that's pretty cool you sorry that's pretty cool in itself. Yeah so I think it's even an improvement over let's say you're in the SVM area of machine learning all you can do is SVM big right there's not a lot of freedom but now you can design models and frameworks and systems and objectives yeah. So I think it's rather than it being lack of intellectual stimulus actually frees you up to do a lot more stuff. Why sunglasses? So you ask this question of course. What are the things to be followed to become a researcher in the AI field? Whatever whatever look I can't predict what's going to happen this field changes so fast no nobody knows what's going to happen so it's probably best you just follow whatever interests you most and if it's a genuinely interesting thing and you like to do it then there will be a place for you to do research on it you know whether it becomes the exact hype of the next years that's that's just chance. Cats or dogs? Cats probably. How long have you been studying AI? I don't know well five years or so I really didn't know it existed when I started bachelor's and then I sort of knew it existed when I started masters and I learned it during masters and then yeah I applied for a PhD at that point I still really didn't know what was going on and only after maybe two three years of PhD I sort of knew what was happening. Why sunglasses? Because it's so bright. What are you working on now? I'm a I have a startup in NLP for legal document understanding well you're not supposed to say understanding it's legal document processing right what are you working oh sorry that was it can you fix my bug yes of course you missed you you added a comma oh no on it like this is commas at the end of things in python will ruin your code if you forget about them because it makes whatever you whatever you have into a tuple and that's quite messy to find later. Who do you think is better Messi or Ronaldo? I have not cared which Ronaldo even I don't know I actually when I watched soccer when Ronaldo was still playing but I have no clue. Most problems in deep learning oh we already had that what are your opinions on Vtubers? I honest I have never watched a Vtuber other than looking up what is a Vtuber if you know I have no I have no basis to complain since I'm wearing gimmicks when making videos so you know they are yeah they're fine I guess whatever they want to do. Any tips any tips could help in reading and understanding research papers not really it just I think it comes with practice and understanding that most people in their papers they they sort of want to sound more they want to sound better or more they want to sound more relevant than they are so they'll frame their work in very formal you know prosaic ways and you just have to keep in mind that their ultimate ideas are going to be there's like very few ideas per paper you try to read those out try to see a little bit what's the real meat behind a paper and yeah the rest comes with with practice. Are ANNs like real ANNs so are artificial neural networks like real neural networks well in some ways they are in other ways they're not we've certainly come a long way from the the real neural network though I think even if you know the individual neurons aren't like real neurons and backprop doesn't exist in the brain and so on the things that we learn about distributed representations of data and things like that can can definitely carry carry over from from either field so even even though there might be sort of the lowest layer has diverged there are aspects of the emerging phenomenon that might be quite the same. Advice for undergraduate deep learning researchers if you're a researcher and you're an undergrad you're already you're doing fine like I when I was an undergrad I had trouble avoiding walls if you're already a researcher you know good job that you don't need my advice. Favorite non ML book? Well I don't know that's that's tough honestly I am not much of a of a book person so I don't even have a favorite ML book I like I can like I know some but I've never read you know an actual book yeah so sorry no good answer on that oh no I lost I lost I lost all of you I'm sorry I'm I will have to skip some of the some of the question if I've skipped yours please ask again the chat has just scrolled down so I'll I'll do these ones and then I'll go up if there are not more ones I'm sorry. Is the AI field saturated? I don't know like sure there is like super duper increase in research papers and that seems a bit saturated and also there is sort of a lack of big new inventions and all of that but on the other hand there are lots of products yet to be built on these technologies so I don't think necessarily the the field itself is saturated maybe maybe the core research path we're on right now is due for a bit of a disruption a bit of a new paradigm a new idea or so but you know favorite movie yeah what do I know Lord of the Rings or something I like Lord of the Rings good movie good movie also Fight Club. Pytorch versus TensorFlow what's your perspective just use whatever like whatever makes you happy bickering about bickering about libraries and bickering about libraries and programming languages and so on is too often not worth it use whatever solves your problem the easiest or or fits your situation what do you think about prompt engineering probably probably going to become important in the near future I think we still don't understand I think there are still a lot of tricks in prompt engineering that we haven't figured out quite yet just because it's so young right why do you think we focus on big data and scalability instead of training efficiency is it just Google I guess it is probably just Google it's easier right yeah it's it's way easier to just add machines because they have money they have lots of money and as far as we can tell scaling up gives you an almost guaranteed improvement in performance and efficiency is harder because you know the I don't know the people who designed the core libraries like CUDA and and and and and Blas and Lopak they already did a lot of efficiency built in and sure we can we can make things a lot more efficient but I think it's just it's easier to just scale up especially if you're big companies what are your advice and tips for applying a PhD and AI at ETH do you need to have publication record what is the acceptance rate I don't I don't know I from the time I applied this has changed a lot you know just try to try to be as as attractive as possible to hire that's that's all I can say it's good if you know people like it's good if people in an institution already know you and know that you are capable and competent that helps but other than that just yeah look at what their criteria are and try to match that publications are certainly good nowadays it's it's astounding that you need no you don't need them but a lot of people have them but it's not necessary sorry the other question by Lucas continues is it just Google who steers the field to big data as it's their business politics yeah probably yeah also also it's a competitive advantage right if you're competing with universities you can pretty easily pull ahead by just playing to your your strengths right so you just do if you do research where universities are smaller companies simply cannot pay for it you're essentially you don't have as much competition so but also good things come from that I mean once someone I mean BERT I guess was considered large at the time and now pretty much anyone can train BERT at home or for very cheap on AWS so not sure if if good or bad but certainly they play to their strengths right do you know any other free compute programs other than TRC that could be used to train BERT, Roberto, Electra, yada yada yada I'm I'm not sure so far I've had I've had university resources which was awesome and now I have company resources which are awesome so I'm not I'm not in the situation for free compute do you have innovative idea for AGI okay as as cliche as this sounds but it's probably worth going through like the 80s and 90s and 2000s literature and try to see what people did then but didn't have the compute necessary to achieve but it's it I know this sounds Schmidt-Huber-ish but it's entirely possible that ideas from then or modifications on them come back and combine with the current things that we know and the current compute to to produce really cool things not sure if that's going to lead to AGI but certainly yeah what app do you use in your paper review videos OneNote and OBS to record do you think data scientists should learn Docker, cloud it's overwhelmingly especially for students probably not right so should should is a big word I don't think you should you should probably learn whatever is required for you to do your work I am generally interested in all kinds of technical things so I I learned Docker I learned cloud what not I learned you know whatever I could get my hands on just because I found it interesting and sometimes that helps you right sometimes you can write a better experiment code you can schedule your experiments more efficiently yada yada yada because you're more competent and some other thing but it's certainly not necessary so if you feel overwhelmed do not learn technology outside of the things that you have to know what are your routines for keeping up with the state of the art literature and filtering signal from noise this is an it's an eternal struggle I I like I've never claimed to have the best filter anything I just look around Twitter Reddit archive like the new new releases there various newsletters blogs and so on people posting stuff around on Discord and whatever interests me I look at it but there's there's too much stuff don't you know in your particular niche you should be able to keep up otherwise your niche is too broad so but that's that's sort of your focus if you're a researcher but for the general landscape you cannot hope for more than just grab bits and pieces from from everywhere so yeah would you have done a PhD looking back considering you're doing a startup right now yes I would have I enjoyed the PhD I I've never seen the PhD as sort of a thing that I need to progress or anything like this I don't I don't think a PhD is necessary for a for a career I also didn't aspire to become an academic I simply I did a PhD because honestly I didn't know what else to do and having done it I absolutely enjoyed the freedom and the fact that it placed me like in the middle of this whole happening in research in AI might be that I have done it at the been at the right place at the right time but would totally do it again yeah but not if not as a as a career move there are better ways to do career AI generated meshes and textures based on descriptions when didn't know that was the thing I mean cool how do you see the movement towards decentralization affecting the ML industry is there is there a movement towards decent I think if anything we see a movement towards centralization with the increased requirements in compute like it's not it's not it's not only so it's not only that the compute gets centralized right like okay you now can no longer do your thing on your computer you need to use your cluster and if you don't have a cluster you go to Google or AWS not only is it centralized in that but it goes further it's now you don't even train your own model you simply subscribe to language model API or something like this so I think there is a centralization centralization happening and honestly I don't see something like blockchain interfacing too easily with machine learning because the two things are sort of not only orthogonal they're kind of in opposite directions and it's going to be interesting to see interesting combinations of the two but so far I see a centralization trend in AI is math invented or discovered in Europe who knows it's math invented or discovered I have I've no I have no clue I think math as as any other sort of science is is simply enough so in my opinion math is an approximation to the world so it's it's a it's an approximate way of talking about the world and I realize mathematical systems are built upon their own axioms and so on and in theory you could come up with any mathematical system but ultimately we do math because we want to understand and describe the world and yes some people some people are very much off in the in in in abstract space and completely unrelated to the world but I think we as we by we I mean mathematicians humanity keeps math reasonably close to reality so such that my opinion is it's it's an approximate description of of reality in the same way as physics is and the same way as biology is and so on when will you do a full review of open AI codex I'm playing around with it I'm trying to to see what it can and can't do and so on also there I think people haven't haven't quite figured out yet what the people haven't quite figured out yet what what things are possible but neither have I so yeah when are you giving Kondi again it's happening every every Wednesday and Thursday at five do you think RL is under appreciated what what do I know probably probably not it's I think it's RL is appreciated quite well for essentially for essentially being random search it's it's retained it's RL is more like the this idealistic idea that there's an agent and it does nothing else than learn from observation and reward so and I think this this captures the mind of many people when they think of AI so I think it's appreciated quite well your advice for pH applying for PhD and ETH yeah just try to match try to match the criteria it's good if you know someone that's that how to get into research as a beginner I'm self-taught don't know what to do to get into research just if you're if you're really a beginner I would try to just focus on a single area like just pick an area that particularly interests you like a sub sub sub field of AI and just try to get into that you might choose that a bit smartly such that you know it's not how do I build GPT-3 but you know any anything where you can do some things yourself then try to get a good overview over that subfield and and I think once you have that you already have a lot of prerequisites to understand the whole field if you're a master student now would you still go for a PhD yes definitely how much do you bench or squat I don't I've never done single rep max so I don't know bench maybe I'm not good at bench maybe 60 kilo squat I don't know 120 many areas of ML research are fascinating so how do you decide which research problem to work on yeah so this is a combination of whatever interests you and whatever do you whatever you think so what interests you what you're good at and what's you know possible physically let's say given your your your resources so it's a bit of a strategic selection but ultimately you should go with what you're what you're interested in because you know if it doesn't work out at least you've done something that you thought was cool and you know if you if you go for a topic that doesn't interest you if it doesn't work out then there's really nothing good about that do you think no I'm not saying saying these words no mood to get kicked off YouTube you think weather forecasting will be possible in the future no you think we could create a GI in this lifetime it depends on how you define a GI I I have no I have no idea for most definitions you'll probably never reach it you think having a PhD is important for pursuing research in ML no you can do ML well completely without having a PhD your opinions about the computer vision lab I have vaguely heard like I don't know many people from there so far looks looks neat looks cool and they do interesting stuff graph neural networks versus convolutional neural networks whatever fits your probably everything the CNN is a special case of the GNN right so I don't know if you like special cases go for a CNN how can we call minimax algorithm is AI or not sure like we call if conditions AI so we should also be able to call minimax conditions AI okay now I've I've skipped a bunch of questions again so the chat refreshes periodically and I'm kicked out I'm very sorry if I skipped your question what for is your green background to to to to project stuff on it I can do I have filters here filters effect probably chroma key no there we go better why capsule networks didn't go further because it's a it's a good idea but it's it doesn't like there's a lot of magic and whenever your your model needs magic there's just a small chance that it's going to work but probably not so this one didn't plus it's it's computationally at least the way it was formulated at the beginning it's computationally not not really nice to current hardware how have you learned the math for ML you pick it up as you go like I don't think it's worth learning all the math there is and then going into ML just go into ML look at what's the math required to understand the things that I need to understand and then learn that area of math so math is too vast to try to pre-learn it did you ever do you ever talk with Lex Friedman not yet not yet divide looked at something sorry chat no okay this is this is too fast for a single human to follow sorry where were we hmm yeah I'm totally I'm totally sorry for skipping for skipping questions I'm overwhelmed I need I need some I'm saying I need some sort of automatic model to answer the questions for me no do you think love will save that's a Lex thing no that is a Lex thing totally totally can you share your thoughts on how one comes up with research problems or topics even smartly throwing spaghetti at a wall and seeing what sticks may fail or bear fruit yes it's a science in itself I guess and technically the senior people in research should be able to help you with that so this I think mostly comes with experience from research in that you have to develop a taste for what ideas are in the reach of the current state of knowledge so are not known yet but within reach and also likely to be possible this is way harder to do if you're new to the research field if even if you're super duper competent this is a bit of a taste that you acquire over a career in in research so try to maybe ask ask around people who have been around for longer or try to study papers of the past and sort of try to estimate what kind of steps they made over previous work and then you can get an idea of how far you can look ahead over the current edge of knowledge should I pursue prompt engineering sure if you're if you if you like it that's I mean it's it's it's a cool new topic right what's your favorite chai I with milk is that is that a is that a thing or do all chai have milk I'm not exactly sure did you ever consider going to a big tech company to do a research Google open AI Facebook I I was part of Google as a research consultant and an intern during the PhD and it's it's definitely a cool place to do research now I've decided that I'd rather do do a startup it's a different climate it's a different climate to be in a big corporation or or a small one all of them have their advantages and disadvantages yeah that's it's mostly a matter of of taste and you know experiencing both things I think is a is nice so if you've done one maybe give the other one a shot have you come across research regarding data set size versus data set quality I think there is I'm not sure if someone or or if there's a lot of research looking at this specifically but for sure we see that both are both are quite important so you know like these language models they over sample corpora that they know or qualitatively you know high quality so you can make up in a certain in a certain manner for lack of size with quality or possibly vice versa and there there is research on the sort of adversarially created data sets where they create the data points in such a way that you only need like one data point per class right so they could sort of condense the data set into single data points and then that's your training set like 10 data points for 10 classes and then still the classifier generalizes so that gives a hint that if your data is qualitatively quite high it can replace size but of course that's an artificially contrived example so I've come across research like this but I don't think there's a definite answer as to how important each of the factors are what is the best way to get started with using transformers well I don't know go to hugging face do a tutorial this is all possible in colabs now so a browser is probably all that all that you need what framework do you suggest for distributed or parallel inferencing for sd based object detection models on nvidia gpu I've I have no idea I've never done object detection in inference mode or anything like like I've never done object detection beyond research so don't I'm really bad person to take advice from here in your opinion is neuro symbolic BS probably not right probably there's something to it because if you if you introspect your own head you you clearly see that you yourself are sort of manipulating symbols also the fact of how we program right we manipulate variables and so on we developed programming languages to also conform to our own thinking patterns and patterns and so on so I definitely think that something symbolic is going on in the brain and I've no I've no idea whether that is an illusion and it just arises from neural networks doing their thing or whether the brains architecture has something built in to support that and we should also augment artificial neural networks by something like this I don't know but it's it's certainly not it's certainly not BS yeah I the question is whether it arises naturally and then appears to the thinker as symbolic or whether there are fundamental structures that you need in order to enable the symbolism in a what is essentially distributed hardware leticia is here nice crossover hi leticia any advice to non ml people for doing ml no you're not you're not too far behind like ml changes so fast nowadays that if you're a non ml person you're not very far behind ml people that is if you are working with the current deep learning based systems and so on if if it comes to it like if you're doing you know class classifiers from scikit-learn things like this xg boost and so on then you probably want to get reach out to a person who has been doing this for a long time and get some advice from them how was your application process for a PhD I did my master thesis with the professor that ended up hiring me for a PhD so the lab already knew me and they had openings at that point so it was relatively smooth and it was just before the big the big boom so I think had I been later I might not have been necessarily selected or at least I would have had a severe competition whereas I don't think I had like the circumstances were the circumstances were very fortunate for me I have to say do you recommend going from a bachelor's to PhD directly what do you think about EU academia I mean if you're looking for an academic career or so on it sure it's the American model right to do bachelor's and then then then PhD I don't know sometimes it's not possible like here at ETH it's it's not necessarily possible there is a special program fast track PhD but in essence you do need a master's to get into a PhD depends on your in your school yeah what's your favorite optimization technique Adam used to be RMS prop because I did like at some point I did like three consecutive projects were always RMS prop would be a little bit better so I was like RMS prop is the best always but yeah I like just throw something at it and see what see what happens you can't you can't really go wrong with with applying Adam and doing a little bit of tuning for a learning rate yeah that's that's all already do heuristic like try a bunch of learning rates I've never used genetic algorithms in in any serious way so okay chat refreshed again sorry from a scale of zero to ten how much did you suffer during your PhD well there are definitely there are definitely times when it gets more more more strict like conference deadline times and so on things when you're stuck times when your stuff doesn't work though I have generally enjoyed the PhD so I don't want to complain yeah that's that's that do you think fairness and bias in a eyes important is as important as we think now why do we expect a to be unbiased if every intelligent being is biased well yeah okay that I guess you can you can write an entire essay about this I do think fairness and bias research is definitely warranted and important because they're like there are many there are many problems with this I think you know since we're not doing AI just out of joy I guess maybe the researchers are but ultimately we want to build tools that make our lives better and you do expect from a tool that it works in a certain way that is reliable that it is you know sort of works as advertised right and in this case you should be at least aware if there is some sort of a bias in the tool right if there's a product that some company sells you know it's it's good if it if it was unbiased in all ways but at least you should be aware if it was so on so you know with respect to that I definitely think this is this is important I do I do think the fairness aspect might be a bit overhyped in the sense that I see a lot of papers that simply reframe whatever they're doing in the language of fairness or or de-biasing or something like this and in hopes of getting their papers published because it's kind of a hype topic right now and I also think there there are there are a lot of there are a lot of I don't know pointless things done down there or yeah things like this but I definitely don't think it's not important that's what I would like to say yeah but sometimes I wish that that community was a bit more like a bit more sober a bit more neutral takes on things rather than the very opinionated takes that a lot of these papers tend to have why are transformers better than fully connected networks well they're not they're probably better for some things and and then the mixers came back and whatnot it seems that the actual architecture might be less important than the fact that you have lots of parameters and somehow shuffle the data around yeah oh Connor is here thank you Connor thanks a lot this this yeah people who don't don't oh sorry Connor is a Henry AI labs if you don't if you don't know Henry AI labs you should definitely also subscribe there I think we we we complement each other quite a bit there and in fact there's probably space for a lot more of us youtubers in the AI space because there's just so much stuff like I think Connor would agree we can't we can't possibly keep up with everything that's happening it's crazy are transformers computationally more complex than convolutional models yes I mean you can you can you can debate you know which one uses more compute and more parameters but just from a perspective of of what you need to do in a convnet you it's simply a linear operation with fixed weights and in a transformer you first have to produce the weights to do the forward propagation you have to produce them in a dynamic way so that's like one more step that you need to do in each layer yeah do you think that deep learning architectures for graph processing have a future yeah sure I mean it's probably a hardware constraint a little bit right now for graph processing because it just doesn't lend itself a lot to current matrix multipliers that expect to have sort of very rigid structure tensors but a lot of problems are graph structured and not easily structured otherwise like as tables or images or sequences or whatnot so definitely graph processing it remains very important do you have tips to address burnout in ML no chill I think FOMO is FOMO is big in in ML because there's so much stuff happening right and yeah just relax like if you've ever if you've ever shut down for a week or something you realize that you know you're still alive and new things will still keep appearing even though you've missed a bunch of things so you know slow down and do your thing that's probably a good things who disliked the video who disliked the video it's fine it's fine it's cool dislikes are good I like the dislikes like I youtube discusses always to remove the dislike button I enjoy the dislike button even like on my videos this is the best signal you know sometimes I think I did a good job with this video I did a bad job with this video but you know you see it the likes are not as much an indicator as the dislikes the dislikes are like the best indicator of whether or not I'm doing a good job and so I appreciate everyone who clicks the dislike button if they are unhappy with the with the results do you think there hasn't been research do you think why do you think there hasn't been research into better deep learning initializations that allow faster conversion there there is there's like massive research into initializations still ongoing but there is a lot of research into into initializations like yeah are you into crypto I like crypto like I like the idea of of of crypto yeah it's a cool topic it's a I'm not sure like yeah because you instantly want to combine it with AI but I don't see a straightforward way and yeah other than please pay me for my computer or my gradients or something like this who knows 12 dislikes within seconds thank you thank you do you like roasty no I okay I don't like potatoes too much yeah except if they're like small and and and in a frying pan I'm not a big potato fan it's a so here is here's the thing like these a lot of these traditional foods people like oh this is a traditional food of this in this area it's just the famine foods it's just whatever the people during the famines had left to eat right so like woo the potatoes here are really a national dish like no you just you had you you had nothing else like there is other food now I'm not not limited to starch ball well in any case sorry sorry if like yeah and of course I mean I like I like I'll eat the roasty I will enjoy it if it is served to me I would not necessarily cook it myself how does your ideal day look like in terms of being productive I asked myself I asked myself this every single day and I constantly fail so you know ideally I get some work done I get some sports in and I do something for the channel every day that's sort of the ideal day and I don't I don't always like things come up and so on I don't always get to do it but I try what do you find the most awesome in AI I think it's just I think it's quite cool that the barrier to entry is low right in general for computer science topics the barrier to entry is so low like sure you cannot train GPT-3 at home but for many things you need a laptop and that's it right you can learn you can do research you can participate by simply having a laptop and and that I find something extremely cool because in many other research fields this is completely out of question you cannot go into synthetic biology with a laptop I mean you can but good luck yeah so yeah what's your favorite ML technique what do I know no idea probably logistic regression I mean you just take whatever solves the problem and that probably solves the problem most of the time can you cook sushi yes how AI hardware should be large lots good and cheap food for masters and PhD students here the University Mensa has the cheapest food just saying do you work now yes I well I try what do you think about Kaggle I think it's a it's a cool concept I know that very often Kaggle solutions are quite overfit to the particular problem but I see that more as a failure of the competition of a competition proposers than the competition solvers so I think if anything Kaggle is cool and we need to train the competition hosts better you cannot train GPT-3 at home how much would you bet on that yes if you if you don't know yet look up look up Lucas's homebrew NLP project they'll be able to train GPT-3 at home quite quite soon so it's on github it's clash Luke homebrew NLP what is your preferred operating system oh okay um for for phones Android because I'm not sure if the iPhones have widgets by now or so on but Android is just it's more sympathetic to me the iOS is just too too restrictive like I get it to streamline things but it's too much and then for computers I use I use macOS and yes I know I know that's not really popular but as a like it just works like okay I haven't used Linux for a while but it just it's it's a really good UI it just works and I have a Unix shell so yeah I can I can do my my coding and all in the terminal and I have a UI that works for everything else and yeah that's I'm happy with that for servers I prefer something like like Ubuntu just because I know it I know where where things are there so yep share your vimrc file okay I will I will I will share my vimrc file some I can give an introduction to it in some video it's true if your startup fails would you consider moving to Silicon Valley no why what I have been I've been in California a bunch of times now why would anyone live there like I get that in something like the 90s this had an appeal but now it's like the state is super duper dry it's burning all the time there's giant there's like a risk of earthquakes constantly they have rolling blackouts because there are too many people their farmland is degrading heavily so it's not like they're they're gonna be this who were the the farm of the nation for very long anymore they've like giant problems with homelessness and where they haven't there the the the prop the cost of living is so high if you live like in San Francisco or or in the Bay Area I don't I don't get why people still go there like if you're there you're established and so on I sure but actively going there like I think there are just just quality of life wise better better options even if you say you want to go to like the center of what's happening in tech there are probably better options available how do you go through CVPR and Nurev submissions and identify quickly what papers matters I don't like conference okay well submissions are different submissions are a different thing but certainly accepted papers are they're just out of date like they're just they're like half a year old like who cares can NLP automate bureaucracy yes it can I mean some well I'm building a startup based on that premise so are there any significant foundational works in AGI yet I don't know it depends on what you consider AGI who knows no idea sorry like probably probably a significant work will already be defining AGI in a correct way I'm not sure you can consider everything we're doing now significant or foundational to it is a t5 optimization a good thesis topic for a master's degree depends so depends on you know which lab and what you're trying to do after if you're aiming for like certainly it's a good topic right it's cool topic you can do something right and you can you can go and you can say here's a bunch of stuff I found that make a difference and now I'm X much faster or better or something like this if you have the compute available sure you have to be careful what you want to do after if you want to go into like a PhD and you want to go to a professor that requires that sort of so some professors they like their students to be competent in a more theoretical or mathematical way so you'll have to see that in your thesis somewhere you demonstrate a little bit that you're not just like a practical person that that hacks around but that's that's pretty much all I have to say please suggest a good research topic bandits band like they don't die banded research does not die I don't get it but you know I'm hearing computational intelligence more these days what's your opinion in it sure you mean other than you mean instead of AI sure I mean it's probably a more descriptive a better better description of what's going on rather than artificial intelligence computational intelligence because artificial could also mean that like Frankenstein is is franken sorry Frankenstein's monster is also artificial intelligent what was the homebrew link again look it is a Chinese repos sorry it's a it's it's on github it's or you can on our discord we have a channel for a homebrew NLP and the discord link should be somewhere in the description yeah cheap 80k dollars budget lambda labs a GPU system or spend it on cloud if you have the opportunity for if you have the opportunity for a local machine and you use it continuously it's probably better to get a local machine if you can manage it right you also need to calculate that you have to actively maintain it yeah but the costs are are way cheaper if you have it locally sorry sorry C++ for machine learning or just learn a lot of Python you know yeah again programming languages I like don't care but right now it's probably way easier for you if you do learn Python it's not it's not a hard language that I mean it is a hard language don't don't get me wrong I so like really understanding the object model and things like I don't know like when late binding pulls and exactly what order things are resolved and so on like there's a lot to learn in Python but the level you need for doing machine learning it's pretty easy to learn so and and it will be it will be useful rather than trying to stick to C++ not not you know not the least because everyone else's code is in Python and you'll be able to just copy that of course retaining the appropriate license notifications any tips for someone getting into legal tech wanting to apply and machine learning data to legal problems should I focus on NLP anything less obvious that I should know apply to our startup no I mean other than that you know just domain knowledge is important understand what your data understand what you're doing so if you're a tech person try to team up with legal people because it'll save you it'll save you a lot of of headaches of course yeah did you do any work relating to biology in your time at ETH no I have I've I've taken a bunch of life science courses in my bachelor's at the University of Basel I also before that I did a year of studying human medicine so I have some I have some background in biology but I have never done any research in it but I know it's a cool cool new upcoming topic yeah do you have guilty pleasures or are you just studying or working all day yes I waste I waste a lot of time watching YouTube videos on whatever today I wasted like an hour just watching Casey Neistat videos or I watch videos about urban planning or something like like this like I I waste I waste time just as anyone else does so like yeah what do you think about Microsoft brainwaves crypto mining sounds legit is this legit I mean this might be legit I don't even know I don't even know at this at this point do you think we can reach AI by gradient descent or do we need new learning algorithms well in gradient descent it certainly might have its part right it's a good optimization algorithm for for neural networks so yeah who are your top three AI researchers I you know like Schmidt Uber has this new blog post where he calls look up Benjo and Hinton LBH I just thought that's a cool acronym like LBH but I don't I don't know I don't have favorite favorite researchers you know there are there many more than three that do cool work do you play any games I I used to I used to play games I no longer have time for playing games so yeah could you upload this AMA on your channel I haven't said anything outrageous so far so I actually might you know it's always a bit yeah a bit a bit with the live things especially when I show my screen I never know like have I typed something into my browser and then an auto suggest had like some search from I don't know when where I was wondering hmm how like how does this very gory thing work or something so you never you never know yeah but given that this is just this I might sorry why is genetic algorithms not more popular well I think it's well first of all it's not suited to the hardware we have and I don't mean just GPUs genetic algorithms need super duper duper massive parallelism because usually they happen in populations of living beings with I don't know thousands or millions of individuals at the same time recombining and developing with different pressures from the environment and so on and and that is just not going to happen on a computer that essentially does sequential computations or minimal parallelism even on a GPU do you always wear glasses yes they're stuck yeah are you still active in us that said what sports do you do else I'm yes I'm still active that's that pretty much covers my sports needs at the moment other than that I just lift sometimes Lex asking it what's what is your meaning to life I have no idea no clue what is the story of the big sunglasses I don't know I just I thought I'd wear them and now people ask that question and I think it's funny that people ask the question so I keep wearing them why don't you use night but in your chat I don't know what night but is I am terrible at tech I guess what university did you graduate I have a bachelor's degree from the University of Basel in Switzerland and I have a master's and a PhD from ETH in Zurich what do you think of the learned hyper parameter optimizer paper and why does it get zero news coverage I have not heard of it maybe because it's gotten zero news news coverage but usually just just usually I don't know the paper but usually the meta learning frameworks and so on they sort of they shift the problem right they shift the problem from optimizing a problem to optimizing the optimizer and usually they run into the same problems when optimizing the optimizer and it happens to work for the papers or the techniques they look at but then very often the trade-off of using such a system aren't worth it because they often don't work as well as advertised plus you still have the problem of having to now optimize or hyper parameter search inside of your optimizer yeah even if the papers say you don't have to do that you quite often do do you think Sanskrit is good for AI I'm not sure what what you mean like to research on Sanskrit or I'm not I'm not sure about the question have you been in Poland I have been in Poland yes I have visited Poland before cool country very very there's sort of a there's also a bit of a like I experienced the Polish as being very proud of Poland or being being Polish and and and in general they have they think yeah Polish things are cool and you know that gives it gives the gives the people a I don't know a nice a nice attitude because they're well they're in Poland so they're they're a bit you know already a bit happy about that which is cool what do you do for your day job I am a I yeah program mostly for my own startup what do you think about geometric deep learning well given that haven't we haven't we just released like a giant street talk episode but is this no am I saying too much here no we have right yeah it's I mean it's it's it's certainly certainly cool like this this pushing pushing it's it's time we push the deep learning to to new modalities and so on be that graphs or be that you know other manifolds and whatnot there's an entire problem space out there that is not accessible by the classical methods of just assuming Euclidean geometry continuous flat spaces and so on and therefore yeah I'm a big fan how much of group theory should be known for entering geometric deep learning again it's I would I would I would recommend you go read the machine learning papers and then you see what math is required to understand it and then you go read the math rather than trying to go learn all about the math before enter a machine learning field that's probably more efficient have you been in Greece hmm probably probably as a kid probably I've been a long time follower thank you for a contribution thank you for being a follower Mike in your opinion what do you think the next big thing deep learning will be yeah if I knew right I have I haven't I've no idea I can only speculate as as much as all other people can speculate who can tell what the neck next if we knew we do it right so it's probably going to be something rather rather unexpected meh meh I have no idea like there are many people who tell you they know but I'm not I don't I don't believe them don't want to be a professor but love researching should I go for a PhD yes I mean you know whatever whatever is enjoyable to you I think this is just my personal opinion I think people optimize too much for a career the people go like oh I need to do I need to do this masters and then need to do the PhD because then I will do this and this you know life changes and plans change and your interests change and so on so I think you're unless you have an extremely clear plan since you're five years old you're probably better off just doing whatever you're interested in should have a bit of foresight and so on but if you're thinking should I do a PhD or not if you like research if you like the PhD life I can tell you it's sometimes stressful it's rather alone so it's not like you work in as tight teams as an industry and if you find a good advisor that you know you click with certainly that might be something for you but also industry has has appeals and there are positions in industry where you can still do quite a bit into the researchy direction you think consciousness in AI will occur in our lifetime how would you how would you know right like if you propose it if you like we need a better test probably for consciousness it would be surprising given that we don't even understand our own consciousness like intelligence is probably easier than consciousness no because intelligence you know we sort of measure intelligence with oh which tasks can you solve how fast can you solve these like tasks or new tasks how much can you transfer knowledge how much data do you need for new tasks and so on consciousness we don't even have a clue how to measure it right yeah why is self-driving so much behind in Europe is it I don't know I don't know whether it is probably because Europeans generally are not as risky there is also probably more regulations such that it the outlook of developing a working self-driving system isn't as bright as in other places because say you develop one you would have to probably develop it to a higher safety standard than in other places and once you have it at that standard you it would take a lot longer for the society to sort of to sort of match up with what you're doing right I'm gonna go for like 10 more minutes or so if that's if that's fine what's next for the channel what will take you to one million I don't honestly like I'm not I'm not I'm not I don't plan on on on on how shall I say stopping or anything like I am doubtful that will reach a million I'd be super happy if that happens I'm I'm also pretty happy if it just stays at the at the if it stays at the size it is or if it grows as it has grown now this is this is fine I enjoy this independent of how many people are are watching or how how big it becomes how often did you think of quitting your PhD quite quite a number of times I think that's a common experience though and and then you think like well what else should I do you continue can you please tell me the roadmap to learn deep learning in two weeks that's a good question hmm I have no I have no idea do I probably do do a bunch of tutorials in in computer vision and in NLP in reinforcement learning if you're if you're fast and pick up stuff fast and you'll see sort of the principles what's your view on the machine learning job market in Zurich out of Google is it really mostly the I finance reporting disguised as a mouth probably I have never had to apply in Zurich though I am now an employer so I would be interested in that in the answer to that question as well how do you organize your machine learning projects I don't I'm a mess are you a believer in in what how much can you bench I don't know probably 60 kilos or so I'm not not too good at benching is Jax a good framework to start learning deep learning sure I think Jax is is doing fine yeah sorry is pics to pics or pics to pics HD still the best approach for this tasks are there new approaches I don't know I've I've lost the overview over what's best like what what is best in papers and leaderboards is often not what's actually best in practice you you just need to try out stuff see what other people are doing and that's probably the only way you figure out what's really best and not necessarily from papers so like papers can give you ideas and and new methods to try out but it still comes down to you just apply things to your problem and see what is best consciousness prior versus general intelligence which one do you think is more plausible and achievable I get depends I don't know it depends again on your specifications of what these what these things mean I am like certainly you need some sort of prior built in or not you need but if you do you probably make your life a lot easier learning stuff so I'm not opposed to not opposed to building in priors especially also given that the priors you as a human have aren't perfect right so that already signals you that you don't necessarily need perfect priors in order to in order to be able to to to to build a system that at least displays human level intelligence so yeah most interesting application of AI for good I don't is there is that a like is that a specific thing AI for for good or is it just the concept of using AI for good I'm not sure yeah what would you advise to teenagers really interested in AGI or deep learning research well I don't I don't necessarily think it's relevant that you're a teenager learn anyone interested probably learn to learn to program and then just dive into stuff it's probably most interesting to dive into applications and try to code something yourself try to understand other people's code and from there you can go more a little bit into the theory what's behind it the most underrated skill in ML what do I know I don't know experience probably or maybe nowadays the most underrated skills is knowing how to use random forest and an XG boost and things like this it's it's it's probably still still a lot of problems can be solved using those things favorite and least favorite depiction of AI and ML in fiction I've I don't know that's that's up to the up to the fiction I don't have particular not too much into fiction yeah what's your startup about it's legal tech it's just processing legal documents is transformer in general better than a CNN no in in general it's it's not better in general both have their strengths and weaknesses can someone buy shares of your company not publicly but if you're not opposed to shady backroom deals then you know all up for it have you ever fudged numbers for a paper no but what what you what you do is you just try until you get the number you want right you change you change your method a bit you and so on which is you know I mean that's research you ultimately try to get your stuff to work but there's definitely an element where you put more work into your into optimizing your method than the baselines so and that's just what everyone does internships during PhD good thing yeah definitely cool can recommend tabs are illegal according to pep 8 wow I didn't know I don't I don't like tabs just personal opinion just make your editor do spaces instead of like I use the tab key but make your editor convert it to spaces what are your favorite non ML books and I don't really I don't really have it also it changes I yeah our ML research papers overrated I mean some are in some a lot of probably a lot are probably underrated so how many papers do you read per day I like however many however many come across come in front of me really read one not super many but skim I skim a lot to see if I'm interested four spaces are two spaces I don't I don't know I think I've set my Python to four and my ECMAScript to two yeah are you scared of Google sure in in some way I'm I'm scared of Google I was scared of Google physical security that definitely when I was there they sort of they laugh at you like they smile at you you know the people are actually doing building security they smile at you but they always see you as like a bit of a threat even if you're if you work there right they're like I don't trust you but yeah are we in a simulation no probably not like I know it's a there's a convincing argument but probably probably not what was your dissertation on adversarial examples or in general adversarial learning not not world earth shattering yeah thought on NVIDIA's monopoly it's kind of breaking up no which is cool like AMD is getting in there there are various things like graph core and and and lambda and there's TPUs and yeah yeah it's it's sort of falling apart a little bit though they're probably still make more money than they know what to do with but you know cool thing cool new things coming up in the hardware area how do you do research without burning out you know look after yourself just know yourself and how much how much you can do and you know try to try to not to to not fall into FOMO too much why didn't you go for a postdoc I am not a successful enough academic to do to become a professor so doing a postdoc makes no sense for me yeah I just like I haven't I haven't I haven't hyper published papers during my PhD I also don't have any super impactful papers or anything so becoming a professor is not really in it for me nor would I have necessarily liked to do that how do I contact you about deep judge my company's working on something similar and we have a prototype in beta as well a lot of users and firms interested answer me you go to our website deep judge dot AI it's ugly right now we're redesigning it yeah there's a you can you can there's there's an there's an email there's a contact button there otherwise otherwise I'm findable on on Twitter if if you want to step into that cesspool could an AI create a game on the fly like graphic card creates visuals on the fly yes there is already there's work on procedural generation of like game levels so that is definitely that's happening it's still like whether or not the AI can come up with a game like game rules or so on I don't know but it can certainly invent levels of games I've seen I've seen even projects where you can play these in real time so yeah is AI overfunded my sampling bias tells me that it seems like there's no other science except AI for funders I mean there's a bunch of others just like biology and there's a bunch of tech I mean blockchain there's a lot of funding going into AI but there's also a lot of stuff happening right it's a lot of lot of cool products that we haven't built yet given the technologies that we have and that are possible so what's your favorite differential equation the damped oscillator probably video idea neural networks in Conway game of life yeah why not I thought about a neural network in Windows Task Manager but I honestly have no idea how to achieve that in what order would you prefer work as a research scientist DeepMind NVIDIA Facebook Google Spotify open air Microsoft probably depends more on your on your team within these companies than on the actual company yeah so just select probably your most enjoyment will come from selecting the people that you're going to work with rather than the company itself have you tried doing ML with Julia I have not I don't know Julia I know some people like it but it seems like right now there's no advantage in it's like a parallel universe so yeah no are you a movie buff best sci-fi movie no I'm I'm not much in I watch movies sometimes I'm not too much into into movies what's the coolest product in your opinion the coolest product I don't know fire the wheel a steam engine I don't know do you know 19 in French what the number Disney is that is an actual number I don't know are you looking for interns um sure I don't I don't know what to do with them so maybe not yeah no not not actively which programming languages are you fluent on Python I used to be fluent in Java C++ bunch of others see certainly but over time I've just not used them and I've just used but oh yeah of course yes so JavaScript I'm pretty fluent in JavaScript as well how to prevent hair fall I have to prevent hair fall do I look like I have advice on that favorite YouTube channels so in general YouTube yeah Pewds PewDiePie obviously obviously yeah no question are you currently unemployed no I'm very employed yeah would you outsource C++ optimization to scale a creation sure if you can't do it merch is coming is coming how many hours do you spend a day during PhD you mean on doing the PhD however many you take do you recommend any advisor for a PhD at ETH this is really a person like advisor is a personal choice like this there's no general recommendation for all people it's just you know you just have to decide what's what's best for you are you on a Switzerland right now yes I'm on a Switzerland correct yeah what's your opinion on Elon Musk I like Elon Musk I know like he over promises stuff and and and so on and he gets a lot of hype going and and whatnot but I'd rather have I'd rather have have that like yeah I'd rather have a world with with Elon than without you know for all of this over promising and whatnot he's also doing really cool things so yeah thoughts on ML and bioinformatics probably coming like probably probably up and coming topic yeah all right so I'm I'm gonna cut this now I as you can see it's dark haha yeah thanks so much for being here as I said this wasn't really planned this it was for 100k I guess maybe we can do AMA's more regularly but thanks everyone for being here thanks everyone for subscribing or watching videos it's a pleasure to make these really and yeah I never would have I wouldn't have guessed that anyone will watch so this it's a it's a huge compliment that you all are here and I really I really thank you yeah thanks a lot and I'll I'll \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The next video is going to be about Facebook's new rebel algorithm, which is an algorithm that solves two player zero sum imperfect information games. Daniel Ngranu, who is a, you know, a longtime poker pro has released these\",\n          \"Spartan trainer is back with a grip and stability workout. Ten burpees, five pull-ups, one minute plank and one minute plate carry are on the agenda.\",\n          \"\\\"I get less competent as I age with respect to coding,\\\" he says. Will there be more live coding sessions in the future? \\\"I'm not really sure\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Integrate the retrieved context into the LLM prompt to generate context-aware summaries."
      ],
      "metadata": {
        "id": "2oMlwbNOCcLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Chunk transcript"
      ],
      "metadata": {
        "id": "ivtyXt2NCnnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=100, overlap=20):\n",
        "    # Split text into chunks of `chunk_size` words with optional overlap\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = ' '.join(words[i:i+chunk_size])\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "fZimaaJaCjjA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Build vector database"
      ],
      "metadata": {
        "id": "q-T7vu-3Cu9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# Get all chunks from all videos\n",
        "all_chunks = []\n",
        "chunk_to_video = []\n",
        "for idx, row in df.iterrows():\n",
        "    video_chunks = chunk_text(row['full_transcript'])\n",
        "    all_chunks.extend(video_chunks)\n",
        "    chunk_to_video.extend([row['video_id']] * len(video_chunks))\n",
        "\n",
        "# Compute embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "chunk_embeddings = model.encode(all_chunks, show_progress_bar=True)\n",
        "\n",
        "# Build FAISS index\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(chunk_embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "eea254ecf1df44d68e46217e9f5af952",
            "40858dae0b3841838c2fe5c8b6809a0c",
            "821b5d974b6444af9f82e9f51b0a0e33",
            "0dbb1feaafd9477396fc93621c878d6f",
            "0e4a184a892f44b9b1ace21a6fc5494d",
            "1d42502afce742cc886a20609dcbf2c1",
            "ab6c05b420284df589d269f2f0e38964",
            "7fdfc417000a402e913e1a31f5d1f10a",
            "63be40ecba7d4a6aac066c902380846d",
            "89fcd453f93e42e9a5af8f413b3260a8",
            "519d20ffa9ff479ba275ee8bece09e8c"
          ]
        },
        "id": "E2hR4GmgCr5q",
        "outputId": "e6a50e2c-e98d-4e52-c3eb-063b1b7f2c5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eea254ecf1df44d68e46217e9f5af952"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define retrieval function"
      ],
      "metadata": {
        "id": "Kn8EnmlQCyk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query_text, top_k=3):\n",
        "    # Encode query\n",
        "    query_emb = model.encode([query_text])\n",
        "    # Search for top_k similar chunks\n",
        "    D, I = index.search(np.array(query_emb), top_k)\n",
        "    # Return the retrieved chunk texts\n",
        "    return [all_chunks[i] for i in I[0]]\n"
      ],
      "metadata": {
        "id": "WLKBpq9KC1uA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Integrate retrieved context into LLM prompt"
      ],
      "metadata": {
        "id": "59ExCWd2C4NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_rag_prompt(main_transcript, retrieved_chunks):\n",
        "    context_section = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"\"\"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.\n",
        "\n",
        "Transcript:\n",
        "{main_transcript}\n",
        "\n",
        "Retrieved context:\n",
        "{context_section}\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "80lYXeLhC7Rw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Run summarization"
      ],
      "metadata": {
        "id": "d8FiIi3KC-vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Example for one video\n",
        "row = df.iloc[0]\n",
        "main_text = get_first_n_sentences(row['full_transcript'], 10) # Get the first 10 sentenece\n",
        "retrieved = retrieve_context(main_text, top_k=3)\n",
        "prompt = construct_rag_prompt(main_text, retrieved)\n",
        "rag_summary = summarizer(prompt, max_length=60, min_length=10, do_sample=False, truncation=True)[0]['summary_text']\n",
        "\n",
        "print(\"RAG summary:\", rag_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_8Bv32IDA6l",
        "outputId": "94fc1893-56d1-4caf-f6de-b4516d5f634c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG summary: A group of over a thousand researchers has been quietly working on their own version of a 176 billion parameter model trained on the nuclear powered supercomputer, the Jonset. You can download multiple size variants all the way up to 176 billion parameters for free. BLUM is an acronym that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Optimize RAG model"
      ],
      "metadata": {
        "id": "JMAg4IStHSQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks = []\n",
        "chunk_to_video = []\n",
        "for idx, row in df.iterrows():\n",
        "    video_chunks = chunk_text(row['full_transcript'], chunk_size=40, overlap=20)\n",
        "    all_chunks.extend(video_chunks)\n",
        "    chunk_to_video.extend([row['video_id']] * len(video_chunks))\n",
        "\n",
        "# Get embeddings for all chunks\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "chunk_embeddings = model.encode(all_chunks, show_progress_bar=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8978c959635749b1974c7b93c411b210",
            "4d513277ff254dbaa936d2d4ab40fd24",
            "f0c6d940bdce4c62b9a2d71a298c0a22",
            "b5524cc9a1bf45378ba4a56d58737cb0",
            "aec8f6342ed243308fb98f0512792f55",
            "f4676029fb754f25868260e5fcfd6bea",
            "f5324a2b7a134c8cb1de518f82d9749c",
            "f11240d1b90e4996b2eb69c14b23208e",
            "08327669f3dc419fb884ca0b1eac4296",
            "e56713b5aada43638189c98e89f8a262",
            "33169022953e46bab1dc68046ce3991b"
          ]
        },
        "id": "NmasrOpgW7Cx",
        "outputId": "c9507e84-d342-4670-fe3e-921c071dd629"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/91 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8978c959635749b1974c7b93c411b210"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_rag_prompt(main_transcript, retrieved_chunks, max_sentences=3):\n",
        "    context_section = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"\"\"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.\n",
        "\n",
        "Transcript:\n",
        "{main_transcript}\n",
        "\n",
        "Retrieved context:\n",
        "{context_section}\n",
        "\n",
        "Limit your summary to {max_sentences} sentences.\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "c3v61dd_xIok"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query_text, top_k=2, exclude_video_id=None):\n",
        "    \"\"\"\n",
        "    Retrieve top_k context chunks for a given query_text, excluding chunks from the same video if exclude_video_id is provided.\n",
        "    \"\"\"\n",
        "    # Build candidate pool (exclude chunks from current video)\n",
        "    candidate_chunks = []\n",
        "    candidate_embeddings = []\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        if exclude_video_id and chunk_to_video[i] == exclude_video_id:\n",
        "            continue\n",
        "        candidate_chunks.append(chunk)\n",
        "        candidate_embeddings.append(chunk_embeddings[i])\n",
        "    if not candidate_chunks:\n",
        "        return []\n",
        "    # Compute embedding for query\n",
        "    query_emb = model.encode([query_text])\n",
        "    emb_matrix = np.vstack(candidate_embeddings).astype('float32')\n",
        "    # Build a temp FAISS index for candidates only\n",
        "    faiss_index = faiss.IndexFlatL2(emb_matrix.shape[1])\n",
        "    faiss_index.add(emb_matrix)\n",
        "    D, I = faiss_index.search(query_emb, top_k)\n",
        "    return [candidate_chunks[idx] for idx in I[0]]\n",
        "\n",
        "# Example usage for one video\n",
        "row = df.iloc[0]\n",
        "main_text = get_first_n_sentences(row['full_transcript'], 3)\n",
        "retrieved = retrieve_context(main_text, top_k=2, exclude_video_id=row['video_id'])\n",
        "\n",
        "print(\"Main text:\", main_text)\n",
        "print(\"Retrieved context chunks:\", retrieved)\n",
        "\n",
        "prompt = construct_rag_prompt(main_text, retrieved, max_sentences=2)\n",
        "print(\"Prompt for LLM:\\n\", prompt)\n",
        "\n",
        "rag_summary = summarizer(\n",
        "    prompt,\n",
        "    max_length=50,   # Adjust as needed\n",
        "    min_length=10,\n",
        "    do_sample=False,\n",
        "    truncation=True\n",
        ")[0]['summary_text']\n",
        "\n",
        "print(\"RAG summary:\", rag_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kfemnMsHVs5",
        "outputId": "da087f8b-694d-4f25-9691-5dc263425a9b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main text: Imagine an AI where all in the same model you could translate languages, write code, solve crossword puzzles, be a chatbot, and do a whole bunch of other crazy things. This sort of an AI would certainly require a supercomputer of hundreds of A100 GPUs and months of training, even on all that power. We would need a team of researchers, the best of the best.\n",
            "Retrieved context chunks: [\"you know since we're not doing AI just out of joy I guess maybe the researchers are but ultimately we want to build tools that make our lives better and you do expect from a tool that it works in\", \"like biology and there's a bunch of tech I mean blockchain there's a lot of funding going into AI but there's also a lot of stuff happening right it's a lot of lot of cool products that we haven't built\"]\n",
            "Prompt for LLM:\n",
            " You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.\n",
            "\n",
            "Transcript:\n",
            "Imagine an AI where all in the same model you could translate languages, write code, solve crossword puzzles, be a chatbot, and do a whole bunch of other crazy things. This sort of an AI would certainly require a supercomputer of hundreds of A100 GPUs and months of training, even on all that power. We would need a team of researchers, the best of the best.\n",
            "\n",
            "Retrieved context:\n",
            "you know since we're not doing AI just out of joy I guess maybe the researchers are but ultimately we want to build tools that make our lives better and you do expect from a tool that it works in\n",
            "like biology and there's a bunch of tech I mean blockchain there's a lot of funding going into AI but there's also a lot of stuff happening right it's a lot of lot of cool products that we haven't built\n",
            "\n",
            "Limit your summary to 2 sentences.\n",
            "\n",
            "RAG summary: Aims to create an AI that can write code, solve crossword puzzles, and do a whole bunch of other things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current summarization model is now acceptable for several reasons.\n",
        "\n",
        "\n",
        "*   First, the entire pipeline from transcript chunking, embedding generation, and vector database construction, to context retrieval and prompt-based summarization is fully functional and debugged.\n",
        "*   The retrieved context for each query is no longer just a repeat of the original transcript but draws from diverse, relevant segments across the dataset, which demonstrates true retrieval-augmented capability.\n",
        "*   The generated summaries are concise, abstract key points, and are not simple copies of the transcript, showing the model is able to synthesize information.\n",
        "*   In addition, the summary outputs reflect both the main transcript and information from retrieved context when appropriate, which is the intended goal of a RAG approach. Compared to baseline summaries, the RAG outputs exhibit clear differences, indicating that the model is indeed leveraging external context.\n",
        "*   This structure is also scalable and can be further evaluated and improved using quantitative metrics or human evaluation in the next phases."
      ],
      "metadata": {
        "id": "0x2ASWhUZ8xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to retrieve RAG summary for multiple examples\n",
        "def getRagSummary(\n",
        "              transcript\n",
        "              ,exclude_video_id\n",
        "              ,n=11\n",
        "              ,top_k=5\n",
        "              ,max_sentences=2\n",
        "              ):\n",
        "\n",
        "  main_text = get_first_n_sentences(transcript, n=n)\n",
        "  retrieved = retrieve_context(main_text, top_k=top_k, exclude_video_id=exclude_video_id)\n",
        "\n",
        "  prompt = construct_rag_prompt(main_text, retrieved, max_sentences=max_sentences)\n",
        "\n",
        "  rag_summary = summarizer(\n",
        "                      prompt,\n",
        "                      max_length=100,   # Adjust as needed\n",
        "                      min_length=10,\n",
        "                      do_sample=False,\n",
        "                      truncation=True\n",
        "                  )[0]['summary_text']\n",
        "\n",
        "  return rag_summary\n",
        "\n",
        "\n",
        "summs = df.progress_apply(lambda x: getRagSummary(x['full_transcript'], x['video_id'],n=11, max_sentences=5), axis=1 )"
      ],
      "metadata": {
        "id": "3L3xfOI4v80R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c4f29d-1810-45b0-c05b-358f7b3e6cb3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:03<00:00, 18.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6.5: Generate RAG summaries for entire dataset"
      ],
      "metadata": {
        "id": "TmgyoLAfOv3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summs = df.progress_apply(\n",
        "    lambda x: getRagSummary(\n",
        "        x['full_transcript'],\n",
        "        x['video_id'],\n",
        "        n=11,\n",
        "        max_sentences=5\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save into df\n",
        "df['rag_summary'] = summs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhA94vO9O2ee",
        "outputId": "f1288496-8fa1-4d32-8682-6210d84694a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:39<00:00, 21.96s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Evaluation: ROUGE + BLEU"
      ],
      "metadata": {
        "id": "fFZyctKzD4sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z1fdUq5AV4P",
        "outputId": "3644e9ea-7c0a-408a-c6ae-6ac2a50298c6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(\n",
        "    ['rouge1', 'rouge2', 'rougeL'],\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Create lists to store scores\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "bleu_scores = []\n",
        "\n",
        "# Loop through all rows\n",
        "for idx, row in df.iterrows():\n",
        "    reference = row['summary']           # baseline summary\n",
        "    generated = row['rag_summary']       # rag summary\n",
        "\n",
        "    # --- ROUGE ---\n",
        "    scores = scorer.score(reference, generated)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    # --- BLEU ---\n",
        "    ref = [reference.split()]\n",
        "    gen = generated.split()\n",
        "    bleu = sentence_bleu(ref, gen)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average ROUGE-1:\", np.mean(rouge1_scores))\n",
        "print(\"Average ROUGE-2:\", np.mean(rouge2_scores))\n",
        "print(\"Average ROUGE-L:\", np.mean(rougeL_scores))\n",
        "print(\"Average BLEU:\", np.mean(bleu_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qm0aIsQAEco",
        "outputId": "8a135a51-afcc-4347-c515-393b5e7b8a17"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1: 0.3780841199203349\n",
            "Average ROUGE-2: 0.28416942269777873\n",
            "Average ROUGE-L: 0.33932818468841897\n",
            "Average BLEU: 0.2328225947892693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 3: Human Evaluation\n",
        "(Check out Final Report for more information)\n"
      ],
      "metadata": {
        "id": "jwA7bDmIJ_A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4: Model Fine-Tuning"
      ],
      "metadata": {
        "id": "VerlymJtKTlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic fine-tuning (T5-small)"
      ],
      "metadata": {
        "id": "JbhR-E9KLMVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3rcZjEJLVQA",
        "outputId": "c595020b-863f-4590-f556-06237bba4604"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Build the Training DataFrame"
      ],
      "metadata": {
        "id": "qJ_NccZSNcBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training dataset for fine tuning, using RAG summary as target\n",
        "# Keep only rows where rag_summary exists\n",
        "train_df = df.dropna(subset=['rag_summary']).copy()\n",
        "\n",
        "# Create input and target columns\n",
        "# We can choose either full_transcript or a truncated version\n",
        "train_df['input_text'] = train_df['full_transcript'].apply(\n",
        "    lambda x: get_first_n_sentences(x, 80)\n",
        ")\n",
        "\n",
        "train_df['target_text'] = train_df['rag_summary']\n",
        "\n",
        "# Show example\n",
        "train_df[['input_text', 'target_text']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7tCuqt3PLh7q",
        "outputId": "da850f30-0a70-4082-e1e4-6a4b58340800"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          input_text  \\\n",
              "0  Imagine an AI where all in the same model you ...   \n",
              "1  What's going on guys welcome back to another S...   \n",
              "2  You know in the coming months, leadership at e...   \n",
              "3  Oh, I'm building this hat. Look how good it is...   \n",
              "4  Good morning! We are up to day 14 of Reps For ...   \n",
              "\n",
              "                                         target_text  \n",
              "0  A group of over a thousand researchers has bee...  \n",
              "1  You are an expert summarizer. Given the follow...  \n",
              "2  You are an expert summarizer. Given the follow...  \n",
              "3  You are an expert summarizer. Given the follow...  \n",
              "4  You are an expert summarizer. Given the follow...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3adacb6-88ef-4014-99bb-274f861abf96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Imagine an AI where all in the same model you ...</td>\n",
              "      <td>A group of over a thousand researchers has bee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What's going on guys welcome back to another S...</td>\n",
              "      <td>You are an expert summarizer. Given the follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You know in the coming months, leadership at e...</td>\n",
              "      <td>You are an expert summarizer. Given the follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oh, I'm building this hat. Look how good it is...</td>\n",
              "      <td>You are an expert summarizer. Given the follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Good morning! We are up to day 14 of Reps For ...</td>\n",
              "      <td>You are an expert summarizer. Given the follow...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3adacb6-88ef-4014-99bb-274f861abf96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3adacb6-88ef-4014-99bb-274f861abf96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3adacb6-88ef-4014-99bb-274f861abf96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d84674e5-d487-4f47-91cc-bf771e6a770e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d84674e5-d487-4f47-91cc-bf771e6a770e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d84674e5-d487-4f47-91cc-bf771e6a770e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_df[['input_text', 'target_text']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What's going on guys welcome back to another Spartan trainer and as of today Officially one week out so this is I think one of the last three or last four training sessions that I'm doing It's a grip and stability workout. You probably noticed I'm sitting on the ground right now. I don't know I'm in a bit of warm-up position. I'll show you how to do it It's one of my favorite warm-up exercises. So what you want to do is get your butt next to the wall I start sidewards and put my legs I rotate them up against the wall and then I kind of move my butt closer to the wall and Put my legs into like a squat position And now the idea is get your butt right up press against the wall as close as you can It's going to be really tight on your hips and and your glutes and your ankles and your knees But it's it's a really great stretch to get your lower body Primed for the exercises you're about to do in the gym and it can be can be for any workout Because I mean your hips come into play with everything and you can also set up bending your knees put your legs Straight up against the wall and spread them out like kind of in a in a split position And so I alternate between both these I'll try to hold them for about a minute But what do we have on today's workout agenda? Well, it is a strength and stability workout or grip and stability workout So first exercise we're going to start off I'm going to start off with a circuit and we're also going to do two circuits today Actually, it's\",\n          \"Good morning! We are up to day 14 of Reps For Rona. And you know what that means? A lot of people who were coming back into the country, like for example my friend Justin, we were having a conversation this morning, he had to go into self-isolation for two weeks. And since we're up to day 14, that means two weeks. Two weeks of doing Reps For Rona. If you've been following along for every day, you've been doing the movement, if you're watching this in the future, this series is doing 10 minutes of movement every day while the coronavirus is alive and taking over the world basically and everyone's indoors. It's paramount that even though you're indoors and you're, a lot of people are on house arrest, it's that you keep moving. What does it do? Helps boost your immune system and plus it's fun. So today on the menu is we are adjusting as needed. The last few days we've done a lot of tense exercises. I explained this in a previous video about life being like an accordion, same with the body. Some exercises tighten the body up and other exercises loosen them. And so today we're adjusting as needed. I woke up this morning feeling a little bit tight and it's still morning, it's almost lunchtime here really, but this is the theme today is adjusting as needed. This is going to be 10 minutes of wake up stretches. I like to get moving as soon as I get out of bed or really just anytime I've been inactive. So whether you're just getting out of bed or you just decided you want to get moving for 10 minutes, these are some stretches slash movements that you can do anytime of the day. But let's get started. We've got 10 minutes on the clock. We'll start off with the lunge kneel, then we'll do the shin underbody, Jefferson curl, squat reach, and then arms crossed underbody. So let's get started. Whole body, we're trying to mobilize here. This is if you're just a little bit too tight, you've gotten out of bed and you want to get moving. So let's come to the ground. We're starting with a lunge kneel. Now you can start off like this. This is probably the first stage that you'll get to is just have one knee back, one knee forward. In my case, I'm doing the left side for the first round. If you want to go more advanced, you can put your elbows on the ground. What we're trying to do here is we're not really staying static. We're moving around. So try and look behind your knee and come this way. Look towards your foot. Just trying to get our bodies moving. And now I've set this up to be 30 seconds on, 30 seconds off for the sake of 10 minutes. But this is something that you don't necessarily have to, you can take as much time as you want, especially with mobility. In fact, it feels better the longer you do it. It's like a relieving pain. The next one, we have a shin under body. Remember for the first round of this five exercises, we're doing the left side. So this is where we're going to get our leg horizontal underneath us. Now you might, if you're a bit tight, you might have to come start like this. But if you've had some practice, you can move your leg up, hold your right hand on your left ankle. And we're just going to sit here, go back and forth, looking forward, a deep breath. If you're feeling it in your hip here, try and breathe into that hip there. Again, keep moving. We might even hold these for a little bit into the rest period because that's when most of the benefits start to come. After you've gone through that little bit of, and what I need to stress here is that I'm not holding it in any one position. I'm digging around. I'm adjusting as needed, finding the parts that need a bit more stretching than others. See, that's a nice tight little bit there. So we'll just squeeze my glutes there. There we go. There was a full minute. I've got to check what the next one was. Now, Jefferson curls. This is one of my favorite ones ever. If you have a weight here, you could stand up on the elevated platform and then hold the weight in front of you. But what we're going to do, head forward, touch your chin to your chest. And we're going to bend our spines as if we're bending every vertebrae one by one. And then all the way to the bottom, the slower the better. Touch the ground if you can. We're going to keep going through the rest of this one and then curl back right up to the top. You can also reach out to finish this one off and then come down, hands to your chest, chin on chest again, all the way down, stretching out to the bottom. And now when you're here, you can shake it out like you're a broom, hands are brushing the floor, and then curl right back up to the top. Oh, beautiful. I'm feeling better already. Getting jumping and jiving. And now we have a squat reach. This one's probably a little bit advanced. If you can hold this one here, we're going to reach up. But again, if you can't, just hold a squat like this, reach up to the roof. But I'm going to go right down to the bottom, grab a heel, reach up. Same thing to the other side. Reach up, reach up. Same thing to the other side. Reach up, nice and slow. Deep breaths.\",\n          \"You know in the coming months, leadership at every level is going to be like the audition, the acting audition, the leadership audition for the coming years. And I mean, I'm being serious about at every level. The question or the theme of this Reps for Rona session this day nine is leadership. You know, I read a story of a doctor in New York City. It's kind of like a day by day of what he would be doing. So wake up at 6am, large pot of coffee because of course all the coffee shops are closed and as you could imagine, big day ahead. And then it goes through detailing himself walking through the ward. Basically it's all set up for coronavirus. He has to deal with patients. Every patient he goes to, he's basically assuming that it's something to do with coronavirus. And it paints a really graphic picture of what happens. And so by midday, he's forgotten to even drink water because he's wearing a mask the whole day. And he says, I can see one more patient before he even has a drink of water by midday. How many times have you been working on something that you've forgotten to even drink water? By late afternoon, he realized he hasn't eaten all day because he's been dealing with different patients. And then by 6.30pm in the evening, he's going home, he makes sure to scrub everything, to scrub his phone, to scrub his clothes, to scrub his badge, to scrub his pens, every single thing. It's because he's going home to visit his family, to see his family. When he gets home, his wife meets him in the hallway of his apartment, strips off all these clothes, she puts them in a secure bag, takes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com.\",\n          \"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. We are up to day 14 of Reps For Rona.\",\n          \"A group of over a thousand researchers has been quietly working on their own version of a 176 billion parameter model trained on the nuclear powered supercomputer, the Jonset. You can download multiple size variants all the way up to 176 billion parameters for free.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}